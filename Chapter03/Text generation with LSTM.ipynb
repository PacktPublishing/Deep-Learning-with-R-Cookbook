{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries and reading our file.\n",
    "library(keras)\n",
    "library(readr)\n",
    "library(stringr)\n",
    "data <- read_file(\"data/rhyme.txt\") %>% str_to_lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'jack and jill went up the hill\\nto fetch a pail of water.\\njack fell down and broke his crown,\\nand jill came tumbling after.\\n\\nup jack got and home did trot\\nas fast as he could caper;\\nand went to bed to mend his head\\nwith vinegar and brown paper.\\n'"
      ],
      "text/latex": [
       "'jack and jill went up the hill\\textbackslash{}nto fetch a pail of water.\\textbackslash{}njack fell down and broke his crown,\\textbackslash{}nand jill came tumbling after.\\textbackslash{}n\\textbackslash{}nup jack got and home did trot\\textbackslash{}nas fast as he could caper;\\textbackslash{}nand went to bed to mend his head\\textbackslash{}nwith vinegar and brown paper.\\textbackslash{}n'"
      ],
      "text/markdown": [
       "'jack and jill went up the hill\\nto fetch a pail of water.\\njack fell down and broke his crown,\\nand jill came tumbling after.\\n\\nup jack got and home did trot\\nas fast as he could caper;\\nand went to bed to mend his head\\nwith vinegar and brown paper.\\n'"
      ],
      "text/plain": [
       "[1] \"jack and jill went up the hill\\nto fetch a pail of water.\\njack fell down and broke his crown,\\nand jill came tumbling after.\\n\\nup jack got and home did trot\\nas fast as he could caper;\\nand went to bed to mend his head\\nwith vinegar and brown paper.\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining tokenizer\n",
    "tokenizer = text_tokenizer(num_words = 35,char_level = F)\n",
    "tokenizer %>% fit_text_tokenizer(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words 37"
     ]
    }
   ],
   "source": [
    "# number of unique words in our corpus\n",
    "cat(\"Number of unique words\", length(tokenizer$word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl>\n",
       "\t<dt>$and</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>$jack</dt>\n",
       "\t\t<dd>2</dd>\n",
       "\t<dt>$to</dt>\n",
       "\t\t<dd>3</dd>\n",
       "\t<dt>$jill</dt>\n",
       "\t\t<dd>4</dd>\n",
       "\t<dt>$went</dt>\n",
       "\t\t<dd>5</dd>\n",
       "\t<dt>$up</dt>\n",
       "\t\t<dd>6</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description}\n",
       "\\item[\\$and] 1\n",
       "\\item[\\$jack] 2\n",
       "\\item[\\$to] 3\n",
       "\\item[\\$jill] 4\n",
       "\\item[\\$went] 5\n",
       "\\item[\\$up] 6\n",
       "\\end{description}\n"
      ],
      "text/markdown": [
       "$and\n",
       ":   1\n",
       "$jack\n",
       ":   2\n",
       "$to\n",
       ":   3\n",
       "$jill\n",
       ":   4\n",
       "$went\n",
       ":   5\n",
       "$up\n",
       ":   6\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "$and\n",
       "[1] 1\n",
       "\n",
       "$jack\n",
       "[1] 2\n",
       "\n",
       "$to\n",
       "[1] 3\n",
       "\n",
       "$jill\n",
       "[1] 4\n",
       "\n",
       "$went\n",
       "[1] 5\n",
       "\n",
       "$up\n",
       "[1] 6\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# vocabulary\n",
    "head(tokenizer$word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of 1\n",
      " $ : int [1:48] 2 1 4 5 6 9 10 3 11 12 ...\n"
     ]
    }
   ],
   "source": [
    "# converting our corpus into a integer sequence\n",
    "text_seqs <- texts_to_sequences(tokenizer, data)\n",
    "str(text_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "48"
      ],
      "text/latex": [
       "48"
      ],
      "text/markdown": [
       "48"
      ],
      "text/plain": [
       "[1] 48"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_seqs <- text_seqs[[1]]\n",
    "length(text_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "34"
      ],
      "text/latex": [
       "34"
      ],
      "text/markdown": [
       "34"
      ],
      "text/plain": [
       "[1] 34"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max(text_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting text sequence into an input(feature) and output(labels) sequences\n",
    "input_sequence_length <- 2\n",
    "feature <- matrix(ncol = input_sequence_length)\n",
    "label <- matrix(ncol = 1)\n",
    "\n",
    "for(i in seq(input_sequence_length, length(text_seqs))){\n",
    "    if(i >= length(text_seqs)){\n",
    "        break()\n",
    "    }\n",
    "    start_idx <- (i - input_sequence_length) +1\n",
    "    end_idx <- i +1\n",
    "    new_seq <-  text_seqs[start_idx:end_idx]\n",
    "    feature <- rbind(feature,new_seq[1:input_sequence_length])\n",
    "    label <- rbind(label,new_seq[input_sequence_length+1])\n",
    "}\n",
    "feature <- feature[-1,]\n",
    "label <- label[-1,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'Feature'"
      ],
      "text/latex": [
       "'Feature'"
      ],
      "text/markdown": [
       "'Feature'"
      ],
      "text/plain": [
       "[1] \"Feature\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td>2 </td><td> 1</td></tr>\n",
       "\t<tr><td>1 </td><td> 4</td></tr>\n",
       "\t<tr><td>4 </td><td> 5</td></tr>\n",
       "\t<tr><td>5 </td><td> 6</td></tr>\n",
       "\t<tr><td>6 </td><td> 9</td></tr>\n",
       "\t<tr><td>9 </td><td>10</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{ll}\n",
       "\t 2  &  1\\\\\n",
       "\t 1  &  4\\\\\n",
       "\t 4  &  5\\\\\n",
       "\t 5  &  6\\\\\n",
       "\t 6  &  9\\\\\n",
       "\t 9  & 10\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| 2  |  1 |\n",
       "| 1  |  4 |\n",
       "| 4  |  5 |\n",
       "| 5  |  6 |\n",
       "| 6  |  9 |\n",
       "| 9  | 10 |\n",
       "\n"
      ],
      "text/plain": [
       "     [,1] [,2]\n",
       "[1,] 2     1  \n",
       "[2,] 1     4  \n",
       "[3,] 4     5  \n",
       "[4,] 5     6  \n",
       "[5,] 6     9  \n",
       "[6,] 9    10  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'label'"
      ],
      "text/latex": [
       "'label'"
      ],
      "text/markdown": [
       "'label'"
      ],
      "text/plain": [
       "[1] \"label\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>4</li>\n",
       "\t<li>5</li>\n",
       "\t<li>6</li>\n",
       "\t<li>9</li>\n",
       "\t<li>10</li>\n",
       "\t<li>3</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 4\n",
       "\\item 5\n",
       "\\item 6\n",
       "\\item 9\n",
       "\\item 10\n",
       "\\item 3\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 4\n",
       "2. 5\n",
       "3. 6\n",
       "4. 9\n",
       "5. 10\n",
       "6. 3\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]  4  5  6  9 10  3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "paste(\"Feature\")\n",
    "head(feature)\n",
    "paste(\"label\")\n",
    "head(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot-encoding our label\n",
    "label <- to_categorical(label,num_classes = tokenizer$num_words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of features 46 2 \n",
      "Shape of label 1610"
     ]
    }
   ],
   "source": [
    "cat(\"Shape of features\",dim(feature),\"\\n\")\n",
    "cat(\"Shape of label\",length(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "________________________________________________________________________________\n",
      "Layer (type)                        Output Shape                    Param #     \n",
      "================================================================================\n",
      "embedding (Embedding)               (None, 2, 10)                   350         \n",
      "________________________________________________________________________________\n",
      "lstm (LSTM)                         (None, 50)                      12200       \n",
      "________________________________________________________________________________\n",
      "dense (Dense)                       (None, 35)                      1785        \n",
      "________________________________________________________________________________\n",
      "activation (Activation)             (None, 35)                      0           \n",
      "================================================================================\n",
      "Total params: 14,335\n",
      "Trainable params: 14,335\n",
      "Non-trainable params: 0\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# creating our neural network\n",
    "model <- keras_model_sequential()\n",
    "model %>%\n",
    "    layer_embedding(input_dim = tokenizer$num_words,output_dim = 10,input_length = input_sequence_length) %>%\n",
    "    layer_lstm(units = 50) %>%\n",
    "    layer_dense(tokenizer$num_words) %>%\n",
    "    layer_activation(\"softmax\")\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model %>% compile(\n",
    "    loss = \"categorical_crossentropy\", \n",
    "    optimizer = optimizer_rmsprop(lr = 0.001),\n",
    "    metrics = c('accuracy')\n",
    ")\n",
    "\n",
    "# training the model\n",
    "model %>% fit(\n",
    "  feature, label,\n",
    "#   batch_size = 128,\n",
    "  epochs = 500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate a sequence from a language model\n",
    "generate_sequence <-function(model, tokenizer, input_length, seed_text, predict_next_n_words){\n",
    "    input_text <- seed_text\n",
    "    for(i in seq(predict_next_n_words)){\n",
    "        encoded <- texts_to_sequences(tokenizer,input_text)[[1]]\n",
    "        encoded <- pad_sequences(sequences = list(encoded),maxlen = input_length,padding = 'pre')\n",
    "        yhat <- predict_classes(model,encoded, verbose=0)\n",
    "        next_word <- tokenizer$index_word[[as.character(yhat)]]\n",
    "        input_text <- paste(input_text, next_word)\n",
    "    }\n",
    "    return(input_text)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text generated from seed 1:  Jack and jill went up the hill to fetch a pail of water \n",
      " Text generated from seed 2:  Jack fell down and broke his crown and jill went up the hill"
     ]
    }
   ],
   "source": [
    "# using generate_sequence() to generate text from sequences\n",
    "seed_1 = \"Jack and\"\n",
    "cat(\"Text generated from seed 1: \" ,generate_sequence(model,tokenizer,input_sequence_length,seed_1,11),\"\\n \")\n",
    "seed_2 = \"Jack fell\"\n",
    "cat(\"Text generated from seed 2: \",generate_sequence(model,tokenizer,input_sequence_length,seed_2,11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
