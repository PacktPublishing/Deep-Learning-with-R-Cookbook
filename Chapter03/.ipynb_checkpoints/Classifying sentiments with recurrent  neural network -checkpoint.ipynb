{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting ready to implement recurrent  neural network\n",
    "\n",
    "RNN is a unique kind of network because of its ability to remember inputs which makes it perfectly suited for problems that deal with sequential data like time series forecasting, speech recognition, machine translation, audio and video sequence prediction.\n",
    "\n",
    "In RNNs data traverses in such a way that at each node the network learns from both current and previous inputs sharing the weights over time which reflects that we are performing the same task at each step, just with different inputs.\n",
    "This reduces the total number of parameters we need to learn.\n",
    "Example - If the activation function is tanh, the weight at the recurrent neuron is , and the weight at the input neuron is , we can write the equation for the state , at time t as â€“\n",
    "\n",
    "\n",
    "\n",
    "The gradient at each output depends on the calculations of the current time step and also the previous time steps.\n",
    "For example, in order to calculate the gradient at t=4, we would need to backpropagate 3 steps and sum up the gradients.\n",
    "This is known as Backpropagation Through Time (BPTT).\n",
    "During BPTT while iterating over the training examples, we modify the weights in order to reduce error.\n",
    "\n",
    "\n",
    "\n",
    "RNNs can handle data with various input and output types through different kinds of architectures it supports, which are mainly:\n",
    "\n",
    "One-to-Many : One input mapped to a sequence with multiple steps as an output.Example- Music generation\n",
    "\n",
    "\n",
    "Many-to-One: Sequence of inputs mapped to class or quantity prediction.Example- Sentiment classification\n",
    "\n",
    "\n",
    "Many-to-Many: Sequence of inputs mapped to a sequence of outputs.Example- Language translation, named entity recognition\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting ready..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we're going to use IMDB dataset which contains movie reviews and sentiment associated with it, and we can import this dataset using a built-in function from keras library. Movie reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. Now let us import required library and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb <- dataset_imdb(num_words = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets divide dataset into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x <- imdb$train$x\n",
    "train_y <- imdb$train$y\n",
    "test_x <- imdb$test$x\n",
    "test_y <- imdb$test$y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have as look at number reviews in train and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences"
     ]
    }
   ],
   "source": [
    "# number of samples in train and test set\n",
    "cat(length(train_x), 'train sequences\\n')\n",
    "cat(length(test_x), 'test sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are 25000 reviews in train and test set each, let's look at the structure of train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " int [1:25000] 1 0 0 1 0 0 1 0 1 0 ...\n"
     ]
    }
   ],
   "source": [
    "str(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of 25000\n",
      " $ : int [1:218] 1 14 22 16 43 530 973 2 2 65 ...\n",
      " $ : int [1:189] 1 194 2 194 2 78 228 5 6 2 ...\n",
      " $ : int [1:141] 1 14 47 8 30 31 7 4 249 108 ...\n",
      " $ : int [1:550] 1 4 2 2 33 2 4 2 432 111 ...\n",
      " $ : int [1:147] 1 249 2 7 61 113 10 10 13 2 ...\n",
      " $ : int [1:43] 1 778 128 74 12 630 163 15 4 2 ...\n",
      " $ : int [1:123] 1 2 365 2 5 2 354 11 14 2 ...\n",
      " $ : int [1:562] 1 4 2 716 4 65 7 4 689 2 ...\n",
      " $ : int [1:233] 1 43 188 46 5 566 264 51 6 530 ...\n",
      " $ : int [1:130] 1 14 20 47 111 439 2 19 12 15 ...\n",
      " $ : int [1:450] 1 785 189 438 47 110 142 7 6 2 ...\n",
      " $ : int [1:99] 1 54 13 2 14 20 13 69 55 364 ...\n",
      " $ : int [1:117] 1 13 119 954 189 2 13 92 459 48 ...\n",
      " $ : int [1:238] 1 259 37 100 169 2 2 11 14 418 ...\n",
      " $ : int [1:109] 1 503 20 33 118 481 302 26 184 52 ...\n",
      " $ : int [1:129] 1 6 964 437 7 58 43 2 11 6 ...\n",
      " $ : int [1:163] 1 2 2 11 4 2 9 4 2 4 ...\n",
      " $ : int [1:752] 1 33 4 2 7 4 2 194 2 2 ...\n",
      " $ : int [1:212] 1 13 28 64 69 4 2 7 319 14 ...\n",
      " $ : int [1:177] 1 2 26 9 6 2 731 939 44 6 ...\n",
      " $ : int [1:129] 1 617 11 2 17 2 14 966 78 20 ...\n",
      " $ : int [1:140] 1 466 49 2 204 2 40 4 2 732 ...\n",
      " $ : int [1:256] 1 13 784 886 857 15 135 142 40 2 ...\n",
      " $ : int [1:888] 1 4 712 19 2 2 2 963 2 26 ...\n",
      " $ : int [1:93] 1 4 204 2 20 16 93 11 2 19 ...\n",
      " $ : int [1:142] 1 14 9 6 55 641 2 212 44 6 ...\n",
      " $ : int [1:220] 1 4 288 310 2 7 241 672 4 2 ...\n",
      " $ : int [1:193] 1 75 69 8 140 8 35 2 38 75 ...\n",
      " $ : int [1:171] 1 2 2 2 11 450 7 134 364 352 ...\n",
      " $ : int [1:221] 1 13 28 332 4 274 6 378 7 211 ...\n",
      " $ : int [1:174] 1 13 2 14 33 6 2 2 32 13 ...\n",
      " $ : int [1:647] 1 6 565 255 2 875 103 196 167 2 ...\n",
      " $ : int [1:233] 1 4 86 390 2 520 6 52 2 51 ...\n",
      " $ : int [1:162] 1 13 92 124 51 12 9 13 169 38 ...\n",
      " $ : int [1:597] 1 111 28 2 15 2 475 455 2 9 ...\n",
      " $ : int [1:234] 1 2 2 2 2 2 2 8 4 268 ...\n",
      " $ : int [1:51] 1 806 13 43 161 169 4 875 551 17 ...\n",
      " $ : int [1:336] 1 23 2 2 2 4 254 157 2 7 ...\n",
      " $ : int [1:139] 1 17 19 111 85 2 2 2 201 14 ...\n",
      " $ : int [1:231] 1 14 38 446 2 9 394 13 435 8 ...\n",
      " $ : int [1:704] 1 14 16 4 840 2 20 5 4 236 ...\n",
      " $ : int [1:142] 1 14 22 16 23 522 33 314 54 13 ...\n",
      " $ : int [1:861] 1 2 14 733 2 2 81 24 332 48 ...\n",
      " $ : int [1:132] 1 4 229 18 14 248 2 2 9 38 ...\n",
      " $ : int [1:122] 1 2 9 34 230 61 514 7 32 7 ...\n",
      " $ : int [1:570] 1 11 14 2 2 14 2 50 26 2 ...\n",
      " $ : int [1:55] 1 568 65 9 2 31 7 4 118 495 ...\n",
      " $ : int [1:214] 1 806 21 13 80 2 199 4 114 347 ...\n",
      " $ : int [1:103] 1 54 2 2 2 2 9 2 2 23 ...\n",
      " $ : int [1:186] 1 13 244 2 2 8 97 134 243 7 ...\n",
      " $ : int [1:113] 1 13 165 219 14 20 33 6 750 17 ...\n",
      " $ : int [1:169] 1 159 13 296 14 22 13 332 6 733 ...\n",
      " $ : int [1:469] 1 14 364 2 2 2 47 43 77 2 ...\n",
      " $ : int [1:138] 1 2 168 855 19 12 50 26 76 128 ...\n",
      " $ : int [1:302] 1 101 20 11 63 2 2 46 2 6 ...\n",
      " $ : int [1:766] 1 2 892 711 12 2 38 2 145 11 ...\n",
      " $ : int [1:351] 1 2 13 191 264 146 4 86 5 64 ...\n",
      " $ : int [1:146] 1 13 974 13 69 8 702 930 143 14 ...\n",
      " $ : int [1:59] 1 13 296 4 20 11 6 2 5 13 ...\n",
      " $ : int [1:206] 1 209 888 14 22 47 8 30 31 7 ...\n",
      " $ : int [1:107] 1 13 219 14 33 4 2 22 2 12 ...\n",
      " $ : int [1:152] 1 591 92 851 42 60 104 44 2 14 ...\n",
      " $ : int [1:186] 1 13 258 14 20 8 30 6 87 326 ...\n",
      " $ : int [1:431] 1 4 2 9 149 16 35 221 22 585 ...\n",
      " $ : int [1:147] 1 146 242 31 7 4 2 2 2 451 ...\n",
      " $ : int [1:684] 1 2 8 516 2 93 6 227 7 6 ...\n",
      " $ : int [1:383] 1 2 11 86 2 14 2 2 2 2 ...\n",
      " $ : int [1:324] 1 2 2 322 2 5 68 2 476 2 ...\n",
      " $ : int [1:252] 1 13 286 2 76 5 8 30 2 13 ...\n",
      " $ : int [1:263] 1 48 335 6 337 7 22 2 5 104 ...\n",
      " $ : int [1:787] 1 6 2 2 2 54 49 432 7 2 ...\n",
      " $ : int [1:211] 1 14 20 218 290 4 22 12 16 2 ...\n",
      " $ : int [1:314] 1 49 24 38 2 2 2 10 10 138 ...\n",
      " $ : int [1:118] 1 480 302 18 6 20 7 14 58 6 ...\n",
      " $ : int [1:390] 1 670 2 2 97 6 20 40 14 21 ...\n",
      " $ : int [1:132] 1 2 2 9 2 2 133 177 17 6 ...\n",
      " $ : int [1:710] 1 2 2 7 2 2 645 113 17 6 ...\n",
      " $ : int [1:306] 1 17 210 14 9 35 2 431 7 4 ...\n",
      " $ : int [1:167] 1 2 2 4 327 2 44 14 215 28 ...\n",
      " $ : int [1:115] 1 11 4 402 2 111 7 178 37 69 ...\n",
      " $ : int [1:95] 1 435 8 67 14 17 72 5 61 761 ...\n",
      " $ : int [1:158] 1 31 7 61 118 369 839 14 20 120 ...\n",
      " $ : int [1:156] 1 66 371 2 2 373 21 284 2 2 ...\n",
      " $ : int [1:82] 1 4 2 282 13 69 8 67 14 20 ...\n",
      " $ : int [1:502] 1 14 22 2 236 314 33 2 2 750 ...\n",
      " $ : int [1:314] 1 13 144 2 138 13 520 14 418 7 ...\n",
      " $ : int [1:190] 1 13 92 400 140 46 7 61 96 8 ...\n",
      " $ : int [1:174] 1 61 795 203 30 6 227 7 6 2 ...\n",
      " $ : int [1:60] 1 18 6 20 19 6 114 40 14 13 ...\n",
      " $ : int [1:145] 1 13 219 14 22 2 145 137 780 23 ...\n",
      " $ : int [1:214] 1 11 192 2 2 125 2 2 7 2 ...\n",
      " $ : int [1:659] 1 541 2 517 19 4 2 7 2 827 ...\n",
      " $ : int [1:408] 1 474 66 66 473 8 67 14 20 5 ...\n",
      " $ : int [1:515] 1 2 4 425 410 2 4 876 7 4 ...\n",
      " $ : int [1:461] 1 121 81 13 895 13 473 8 358 14 ...\n",
      " $ : int [1:202] 1 13 66 215 28 2 6 275 22 39 ...\n",
      " $ : int [1:238] 1 827 2 2 10 10 2 2 300 2 ...\n",
      " $ : int [1:170] 1 24 15 76 183 593 11 14 20 21 ...\n",
      " $ : int [1:107] 1 14 20 9 389 10 10 13 16 2 ...\n",
      "  [list output truncated]\n"
     ]
    }
   ],
   "source": [
    "str(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see that our training set is a list of reviews and list sentiment labels. Lets look at the first review and the number of words in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1</li>\n",
       "\t<li>14</li>\n",
       "\t<li>22</li>\n",
       "\t<li>16</li>\n",
       "\t<li>43</li>\n",
       "\t<li>530</li>\n",
       "\t<li>973</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>65</li>\n",
       "\t<li>458</li>\n",
       "\t<li>2</li>\n",
       "\t<li>66</li>\n",
       "\t<li>2</li>\n",
       "\t<li>4</li>\n",
       "\t<li>173</li>\n",
       "\t<li>36</li>\n",
       "\t<li>256</li>\n",
       "\t<li>5</li>\n",
       "\t<li>25</li>\n",
       "\t<li>100</li>\n",
       "\t<li>43</li>\n",
       "\t<li>838</li>\n",
       "\t<li>112</li>\n",
       "\t<li>50</li>\n",
       "\t<li>670</li>\n",
       "\t<li>2</li>\n",
       "\t<li>9</li>\n",
       "\t<li>35</li>\n",
       "\t<li>480</li>\n",
       "\t<li>284</li>\n",
       "\t<li>5</li>\n",
       "\t<li>150</li>\n",
       "\t<li>4</li>\n",
       "\t<li>172</li>\n",
       "\t<li>112</li>\n",
       "\t<li>167</li>\n",
       "\t<li>2</li>\n",
       "\t<li>336</li>\n",
       "\t<li>385</li>\n",
       "\t<li>39</li>\n",
       "\t<li>4</li>\n",
       "\t<li>172</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>17</li>\n",
       "\t<li>546</li>\n",
       "\t<li>38</li>\n",
       "\t<li>13</li>\n",
       "\t<li>447</li>\n",
       "\t<li>4</li>\n",
       "\t<li>192</li>\n",
       "\t<li>50</li>\n",
       "\t<li>16</li>\n",
       "\t<li>6</li>\n",
       "\t<li>147</li>\n",
       "\t<li>2</li>\n",
       "\t<li>19</li>\n",
       "\t<li>14</li>\n",
       "\t<li>22</li>\n",
       "\t<li>4</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>469</li>\n",
       "\t<li>4</li>\n",
       "\t<li>22</li>\n",
       "\t<li>71</li>\n",
       "\t<li>87</li>\n",
       "\t<li>12</li>\n",
       "\t<li>16</li>\n",
       "\t<li>43</li>\n",
       "\t<li>530</li>\n",
       "\t<li>38</li>\n",
       "\t<li>76</li>\n",
       "\t<li>15</li>\n",
       "\t<li>13</li>\n",
       "\t<li>2</li>\n",
       "\t<li>4</li>\n",
       "\t<li>22</li>\n",
       "\t<li>17</li>\n",
       "\t<li>515</li>\n",
       "\t<li>17</li>\n",
       "\t<li>12</li>\n",
       "\t<li>16</li>\n",
       "\t<li>626</li>\n",
       "\t<li>18</li>\n",
       "\t<li>2</li>\n",
       "\t<li>5</li>\n",
       "\t<li>62</li>\n",
       "\t<li>386</li>\n",
       "\t<li>12</li>\n",
       "\t<li>8</li>\n",
       "\t<li>316</li>\n",
       "\t<li>8</li>\n",
       "\t<li>106</li>\n",
       "\t<li>5</li>\n",
       "\t<li>4</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>16</li>\n",
       "\t<li>480</li>\n",
       "\t<li>66</li>\n",
       "\t<li>2</li>\n",
       "\t<li>33</li>\n",
       "\t<li>4</li>\n",
       "\t<li>130</li>\n",
       "\t<li>12</li>\n",
       "\t<li>16</li>\n",
       "\t<li>38</li>\n",
       "\t<li>619</li>\n",
       "\t<li>5</li>\n",
       "\t<li>25</li>\n",
       "\t<li>124</li>\n",
       "\t<li>51</li>\n",
       "\t<li>36</li>\n",
       "\t<li>135</li>\n",
       "\t<li>48</li>\n",
       "\t<li>25</li>\n",
       "\t<li>2</li>\n",
       "\t<li>33</li>\n",
       "\t<li>6</li>\n",
       "\t<li>22</li>\n",
       "\t<li>12</li>\n",
       "\t<li>215</li>\n",
       "\t<li>28</li>\n",
       "\t<li>77</li>\n",
       "\t<li>52</li>\n",
       "\t<li>5</li>\n",
       "\t<li>14</li>\n",
       "\t<li>407</li>\n",
       "\t<li>16</li>\n",
       "\t<li>82</li>\n",
       "\t<li>2</li>\n",
       "\t<li>8</li>\n",
       "\t<li>4</li>\n",
       "\t<li>107</li>\n",
       "\t<li>117</li>\n",
       "\t<li>2</li>\n",
       "\t<li>15</li>\n",
       "\t<li>256</li>\n",
       "\t<li>4</li>\n",
       "\t<li>2</li>\n",
       "\t<li>7</li>\n",
       "\t<li>2</li>\n",
       "\t<li>5</li>\n",
       "\t<li>723</li>\n",
       "\t<li>36</li>\n",
       "\t<li>71</li>\n",
       "\t<li>43</li>\n",
       "\t<li>530</li>\n",
       "\t<li>476</li>\n",
       "\t<li>26</li>\n",
       "\t<li>400</li>\n",
       "\t<li>317</li>\n",
       "\t<li>46</li>\n",
       "\t<li>7</li>\n",
       "\t<li>4</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>13</li>\n",
       "\t<li>104</li>\n",
       "\t<li>88</li>\n",
       "\t<li>4</li>\n",
       "\t<li>381</li>\n",
       "\t<li>15</li>\n",
       "\t<li>297</li>\n",
       "\t<li>98</li>\n",
       "\t<li>32</li>\n",
       "\t<li>2</li>\n",
       "\t<li>56</li>\n",
       "\t<li>26</li>\n",
       "\t<li>141</li>\n",
       "\t<li>6</li>\n",
       "\t<li>194</li>\n",
       "\t<li>2</li>\n",
       "\t<li>18</li>\n",
       "\t<li>4</li>\n",
       "\t<li>226</li>\n",
       "\t<li>22</li>\n",
       "\t<li>21</li>\n",
       "\t<li>134</li>\n",
       "\t<li>476</li>\n",
       "\t<li>26</li>\n",
       "\t<li>480</li>\n",
       "\t<li>5</li>\n",
       "\t<li>144</li>\n",
       "\t<li>30</li>\n",
       "\t<li>2</li>\n",
       "\t<li>18</li>\n",
       "\t<li>51</li>\n",
       "\t<li>36</li>\n",
       "\t<li>28</li>\n",
       "\t<li>224</li>\n",
       "\t<li>92</li>\n",
       "\t<li>25</li>\n",
       "\t<li>104</li>\n",
       "\t<li>4</li>\n",
       "\t<li>226</li>\n",
       "\t<li>65</li>\n",
       "\t<li>16</li>\n",
       "\t<li>38</li>\n",
       "\t<li>2</li>\n",
       "\t<li>88</li>\n",
       "\t<li>12</li>\n",
       "\t<li>16</li>\n",
       "\t<li>283</li>\n",
       "\t<li>5</li>\n",
       "\t<li>16</li>\n",
       "\t<li>2</li>\n",
       "\t<li>113</li>\n",
       "\t<li>103</li>\n",
       "\t<li>32</li>\n",
       "\t<li>15</li>\n",
       "\t<li>16</li>\n",
       "\t<li>2</li>\n",
       "\t<li>19</li>\n",
       "\t<li>178</li>\n",
       "\t<li>32</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1\n",
       "\\item 14\n",
       "\\item 22\n",
       "\\item 16\n",
       "\\item 43\n",
       "\\item 530\n",
       "\\item 973\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 65\n",
       "\\item 458\n",
       "\\item 2\n",
       "\\item 66\n",
       "\\item 2\n",
       "\\item 4\n",
       "\\item 173\n",
       "\\item 36\n",
       "\\item 256\n",
       "\\item 5\n",
       "\\item 25\n",
       "\\item 100\n",
       "\\item 43\n",
       "\\item 838\n",
       "\\item 112\n",
       "\\item 50\n",
       "\\item 670\n",
       "\\item 2\n",
       "\\item 9\n",
       "\\item 35\n",
       "\\item 480\n",
       "\\item 284\n",
       "\\item 5\n",
       "\\item 150\n",
       "\\item 4\n",
       "\\item 172\n",
       "\\item 112\n",
       "\\item 167\n",
       "\\item 2\n",
       "\\item 336\n",
       "\\item 385\n",
       "\\item 39\n",
       "\\item 4\n",
       "\\item 172\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 17\n",
       "\\item 546\n",
       "\\item 38\n",
       "\\item 13\n",
       "\\item 447\n",
       "\\item 4\n",
       "\\item 192\n",
       "\\item 50\n",
       "\\item 16\n",
       "\\item 6\n",
       "\\item 147\n",
       "\\item 2\n",
       "\\item 19\n",
       "\\item 14\n",
       "\\item 22\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 469\n",
       "\\item 4\n",
       "\\item 22\n",
       "\\item 71\n",
       "\\item 87\n",
       "\\item 12\n",
       "\\item 16\n",
       "\\item 43\n",
       "\\item 530\n",
       "\\item 38\n",
       "\\item 76\n",
       "\\item 15\n",
       "\\item 13\n",
       "\\item 2\n",
       "\\item 4\n",
       "\\item 22\n",
       "\\item 17\n",
       "\\item 515\n",
       "\\item 17\n",
       "\\item 12\n",
       "\\item 16\n",
       "\\item 626\n",
       "\\item 18\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 62\n",
       "\\item 386\n",
       "\\item 12\n",
       "\\item 8\n",
       "\\item 316\n",
       "\\item 8\n",
       "\\item 106\n",
       "\\item 5\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 16\n",
       "\\item 480\n",
       "\\item 66\n",
       "\\item 2\n",
       "\\item 33\n",
       "\\item 4\n",
       "\\item 130\n",
       "\\item 12\n",
       "\\item 16\n",
       "\\item 38\n",
       "\\item 619\n",
       "\\item 5\n",
       "\\item 25\n",
       "\\item 124\n",
       "\\item 51\n",
       "\\item 36\n",
       "\\item 135\n",
       "\\item 48\n",
       "\\item 25\n",
       "\\item 2\n",
       "\\item 33\n",
       "\\item 6\n",
       "\\item 22\n",
       "\\item 12\n",
       "\\item 215\n",
       "\\item 28\n",
       "\\item 77\n",
       "\\item 52\n",
       "\\item 5\n",
       "\\item 14\n",
       "\\item 407\n",
       "\\item 16\n",
       "\\item 82\n",
       "\\item 2\n",
       "\\item 8\n",
       "\\item 4\n",
       "\\item 107\n",
       "\\item 117\n",
       "\\item 2\n",
       "\\item 15\n",
       "\\item 256\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 7\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 723\n",
       "\\item 36\n",
       "\\item 71\n",
       "\\item 43\n",
       "\\item 530\n",
       "\\item 476\n",
       "\\item 26\n",
       "\\item 400\n",
       "\\item 317\n",
       "\\item 46\n",
       "\\item 7\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 13\n",
       "\\item 104\n",
       "\\item 88\n",
       "\\item 4\n",
       "\\item 381\n",
       "\\item 15\n",
       "\\item 297\n",
       "\\item 98\n",
       "\\item 32\n",
       "\\item 2\n",
       "\\item 56\n",
       "\\item 26\n",
       "\\item 141\n",
       "\\item 6\n",
       "\\item 194\n",
       "\\item 2\n",
       "\\item 18\n",
       "\\item 4\n",
       "\\item 226\n",
       "\\item 22\n",
       "\\item 21\n",
       "\\item 134\n",
       "\\item 476\n",
       "\\item 26\n",
       "\\item 480\n",
       "\\item 5\n",
       "\\item 144\n",
       "\\item 30\n",
       "\\item 2\n",
       "\\item 18\n",
       "\\item 51\n",
       "\\item 36\n",
       "\\item 28\n",
       "\\item 224\n",
       "\\item 92\n",
       "\\item 25\n",
       "\\item 104\n",
       "\\item 4\n",
       "\\item 226\n",
       "\\item 65\n",
       "\\item 16\n",
       "\\item 38\n",
       "\\item 2\n",
       "\\item 88\n",
       "\\item 12\n",
       "\\item 16\n",
       "\\item 283\n",
       "\\item 5\n",
       "\\item 16\n",
       "\\item 2\n",
       "\\item 113\n",
       "\\item 103\n",
       "\\item 32\n",
       "\\item 15\n",
       "\\item 16\n",
       "\\item 2\n",
       "\\item 19\n",
       "\\item 178\n",
       "\\item 32\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1\n",
       "2. 14\n",
       "3. 22\n",
       "4. 16\n",
       "5. 43\n",
       "6. 530\n",
       "7. 973\n",
       "8. 2\n",
       "9. 2\n",
       "10. 65\n",
       "11. 458\n",
       "12. 2\n",
       "13. 66\n",
       "14. 2\n",
       "15. 4\n",
       "16. 173\n",
       "17. 36\n",
       "18. 256\n",
       "19. 5\n",
       "20. 25\n",
       "21. 100\n",
       "22. 43\n",
       "23. 838\n",
       "24. 112\n",
       "25. 50\n",
       "26. 670\n",
       "27. 2\n",
       "28. 9\n",
       "29. 35\n",
       "30. 480\n",
       "31. 284\n",
       "32. 5\n",
       "33. 150\n",
       "34. 4\n",
       "35. 172\n",
       "36. 112\n",
       "37. 167\n",
       "38. 2\n",
       "39. 336\n",
       "40. 385\n",
       "41. 39\n",
       "42. 4\n",
       "43. 172\n",
       "44. 2\n",
       "45. 2\n",
       "46. 17\n",
       "47. 546\n",
       "48. 38\n",
       "49. 13\n",
       "50. 447\n",
       "51. 4\n",
       "52. 192\n",
       "53. 50\n",
       "54. 16\n",
       "55. 6\n",
       "56. 147\n",
       "57. 2\n",
       "58. 19\n",
       "59. 14\n",
       "60. 22\n",
       "61. 4\n",
       "62. 2\n",
       "63. 2\n",
       "64. 469\n",
       "65. 4\n",
       "66. 22\n",
       "67. 71\n",
       "68. 87\n",
       "69. 12\n",
       "70. 16\n",
       "71. 43\n",
       "72. 530\n",
       "73. 38\n",
       "74. 76\n",
       "75. 15\n",
       "76. 13\n",
       "77. 2\n",
       "78. 4\n",
       "79. 22\n",
       "80. 17\n",
       "81. 515\n",
       "82. 17\n",
       "83. 12\n",
       "84. 16\n",
       "85. 626\n",
       "86. 18\n",
       "87. 2\n",
       "88. 5\n",
       "89. 62\n",
       "90. 386\n",
       "91. 12\n",
       "92. 8\n",
       "93. 316\n",
       "94. 8\n",
       "95. 106\n",
       "96. 5\n",
       "97. 4\n",
       "98. 2\n",
       "99. 2\n",
       "100. 16\n",
       "101. 480\n",
       "102. 66\n",
       "103. 2\n",
       "104. 33\n",
       "105. 4\n",
       "106. 130\n",
       "107. 12\n",
       "108. 16\n",
       "109. 38\n",
       "110. 619\n",
       "111. 5\n",
       "112. 25\n",
       "113. 124\n",
       "114. 51\n",
       "115. 36\n",
       "116. 135\n",
       "117. 48\n",
       "118. 25\n",
       "119. 2\n",
       "120. 33\n",
       "121. 6\n",
       "122. 22\n",
       "123. 12\n",
       "124. 215\n",
       "125. 28\n",
       "126. 77\n",
       "127. 52\n",
       "128. 5\n",
       "129. 14\n",
       "130. 407\n",
       "131. 16\n",
       "132. 82\n",
       "133. 2\n",
       "134. 8\n",
       "135. 4\n",
       "136. 107\n",
       "137. 117\n",
       "138. 2\n",
       "139. 15\n",
       "140. 256\n",
       "141. 4\n",
       "142. 2\n",
       "143. 7\n",
       "144. 2\n",
       "145. 5\n",
       "146. 723\n",
       "147. 36\n",
       "148. 71\n",
       "149. 43\n",
       "150. 530\n",
       "151. 476\n",
       "152. 26\n",
       "153. 400\n",
       "154. 317\n",
       "155. 46\n",
       "156. 7\n",
       "157. 4\n",
       "158. 2\n",
       "159. 2\n",
       "160. 13\n",
       "161. 104\n",
       "162. 88\n",
       "163. 4\n",
       "164. 381\n",
       "165. 15\n",
       "166. 297\n",
       "167. 98\n",
       "168. 32\n",
       "169. 2\n",
       "170. 56\n",
       "171. 26\n",
       "172. 141\n",
       "173. 6\n",
       "174. 194\n",
       "175. 2\n",
       "176. 18\n",
       "177. 4\n",
       "178. 226\n",
       "179. 22\n",
       "180. 21\n",
       "181. 134\n",
       "182. 476\n",
       "183. 26\n",
       "184. 480\n",
       "185. 5\n",
       "186. 144\n",
       "187. 30\n",
       "188. 2\n",
       "189. 18\n",
       "190. 51\n",
       "191. 36\n",
       "192. 28\n",
       "193. 224\n",
       "194. 92\n",
       "195. 25\n",
       "196. 104\n",
       "197. 4\n",
       "198. 226\n",
       "199. 65\n",
       "200. 16\n",
       "201. 38\n",
       "202. 2\n",
       "203. 88\n",
       "204. 12\n",
       "205. 16\n",
       "206. 283\n",
       "207. 5\n",
       "208. 16\n",
       "209. 2\n",
       "210. 113\n",
       "211. 103\n",
       "212. 32\n",
       "213. 15\n",
       "214. 16\n",
       "215. 2\n",
       "216. 19\n",
       "217. 178\n",
       "218. 32\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  [1]   1  14  22  16  43 530 973   2   2  65 458   2  66   2   4 173  36 256\n",
       " [19]   5  25 100  43 838 112  50 670   2   9  35 480 284   5 150   4 172 112\n",
       " [37] 167   2 336 385  39   4 172   2   2  17 546  38  13 447   4 192  50  16\n",
       " [55]   6 147   2  19  14  22   4   2   2 469   4  22  71  87  12  16  43 530\n",
       " [73]  38  76  15  13   2   4  22  17 515  17  12  16 626  18   2   5  62 386\n",
       " [91]  12   8 316   8 106   5   4   2   2  16 480  66   2  33   4 130  12  16\n",
       "[109]  38 619   5  25 124  51  36 135  48  25   2  33   6  22  12 215  28  77\n",
       "[127]  52   5  14 407  16  82   2   8   4 107 117   2  15 256   4   2   7   2\n",
       "[145]   5 723  36  71  43 530 476  26 400 317  46   7   4   2   2  13 104  88\n",
       "[163]   4 381  15 297  98  32   2  56  26 141   6 194   2  18   4 226  22  21\n",
       "[181] 134 476  26 480   5 144  30   2  18  51  36  28 224  92  25 104   4 226\n",
       "[199]  65  16  38   2  88  12  16 283   5  16   2 113 103  32  15  16   2  19\n",
       "[217] 178  32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the first review is 218"
     ]
    }
   ],
   "source": [
    "train_x[[1]]\n",
    "cat(\"Number of words in the first review is\",length(train_x[[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that when we imported our data set, we set the value of the argument <i>num_words to 1000</i>. It means that only top thousand frequent words are kept in the encoded reviews. Let's look at what is the maximum encoded value in our list of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum encoded value in train  999 \n",
      "Maximum encoded value in test  999"
     ]
    }
   ],
   "source": [
    "cat(\"Maximum encoded value in train \",max(sapply(train_x, max)),\"\\n\")\n",
    "cat(\"Maximum encoded value in test \",max(sapply(test_x, max)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above section, we got familiar with that data lets get into details of it. We know that the words of sentences are encoded as per a word index where words are indexed by overall frequency in the dataset. Let us import this word index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = dataset_imdb_word_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the head of word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl>\n",
       "\t<dt>$fawn</dt>\n",
       "\t\t<dd>34701</dd>\n",
       "\t<dt>$tsukino</dt>\n",
       "\t\t<dd>52006</dd>\n",
       "\t<dt>$nunnery</dt>\n",
       "\t\t<dd>52007</dd>\n",
       "\t<dt>$sonja</dt>\n",
       "\t\t<dd>16816</dd>\n",
       "\t<dt>$vani</dt>\n",
       "\t\t<dd>63951</dd>\n",
       "\t<dt>$woods</dt>\n",
       "\t\t<dd>1408</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description}\n",
       "\\item[\\$fawn] 34701\n",
       "\\item[\\$tsukino] 52006\n",
       "\\item[\\$nunnery] 52007\n",
       "\\item[\\$sonja] 16816\n",
       "\\item[\\$vani] 63951\n",
       "\\item[\\$woods] 1408\n",
       "\\end{description}\n"
      ],
      "text/markdown": [
       "$fawn\n",
       ":   34701\n",
       "$tsukino\n",
       ":   52006\n",
       "$nunnery\n",
       ":   52007\n",
       "$sonja\n",
       ":   16816\n",
       "$vani\n",
       ":   63951\n",
       "$woods\n",
       ":   1408\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "$fawn\n",
       "[1] 34701\n",
       "\n",
       "$tsukino\n",
       "[1] 52006\n",
       "\n",
       "$nunnery\n",
       "[1] 52007\n",
       "\n",
       "$sonja\n",
       "[1] 16816\n",
       "\n",
       "$vani\n",
       "[1] 63951\n",
       "\n",
       "$woods\n",
       "[1] 1408\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it is a list of key-value pair where key is the word and value the integer to which it is mapped .Lets see how many unique words we have in our word index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "88584"
      ],
      "text/latex": [
       "88584"
      ],
      "text/markdown": [
       "88584"
      ],
      "text/plain": [
       "[1] 88584"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "length((word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a reversed list of key-value pair of the word index. We will use this to decode reviews in IMDB data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>34701</dt>\n",
       "\t\t<dd>'fawn'</dd>\n",
       "\t<dt>52006</dt>\n",
       "\t\t<dd>'tsukino'</dd>\n",
       "\t<dt>52007</dt>\n",
       "\t\t<dd>'nunnery'</dd>\n",
       "\t<dt>16816</dt>\n",
       "\t\t<dd>'sonja'</dd>\n",
       "\t<dt>63951</dt>\n",
       "\t\t<dd>'vani'</dd>\n",
       "\t<dt>1408</dt>\n",
       "\t\t<dd>'woods'</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[34701] 'fawn'\n",
       "\\item[52006] 'tsukino'\n",
       "\\item[52007] 'nunnery'\n",
       "\\item[16816] 'sonja'\n",
       "\\item[63951] 'vani'\n",
       "\\item[1408] 'woods'\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "34701\n",
       ":   'fawn'52006\n",
       ":   'tsukino'52007\n",
       ":   'nunnery'16816\n",
       ":   'sonja'63951\n",
       ":   'vani'1408\n",
       ":   'woods'\n",
       "\n"
      ],
      "text/plain": [
       "    34701     52006     52007     16816     63951      1408 \n",
       "   \"fawn\" \"tsukino\" \"nunnery\"   \"sonja\"    \"vani\"   \"woods\" "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reverse_word_index <- names(word_index)\n",
    "names(reverse_word_index) <- word_index\n",
    "head(reverse_word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us decode the first review and look at it. Note that the word encodings are offset by three because 0,1,2 are reserved for padding, the start of the sequence and out vocabulary words respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? this film was just brilliant casting ? ? story direction ? really ? the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same ? ? as myself so i loved the fact there was a real ? with this film the ? ? throughout the film were great it was just brilliant so much that i ? the film as soon as it was released for ? and would recommend it to everyone to watch and the ? ? was amazing really ? at the end it was so sad and you know what they say if you ? at a film it must have been good and this definitely was also ? to the two little ? that played the ? of ? and paul they were just brilliant children are often left out of the ? ? i think because the stars that play them all ? up are such a big ? for the whole film but these children are amazing and should be ? for what they have done don't you think the whole story was so ? because it was true and was ? life after all that was ? with us all"
     ]
    }
   ],
   "source": [
    "decoded_review <- sapply(train_x[[1]], function(index) {\n",
    "  word <- if (index >= 3) reverse_word_index[[as.character(index -3)]]\n",
    "  if (!is.null(word)) word else \"?\"\n",
    "})\n",
    "\n",
    "cat(decoded_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us pad the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: 25000 80 \n",
      "x_test shape: 25000 80 \n"
     ]
    }
   ],
   "source": [
    "train_x <- pad_sequences(train_x, maxlen = 80)\n",
    "test_x <- pad_sequences(test_x, maxlen = 80)\n",
    "\n",
    "cat('x_train shape:', dim(train_x), '\\n')\n",
    "cat('x_test shape:', dim(test_x), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us look at the first review after padding it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>15</li>\n",
       "\t<li>256</li>\n",
       "\t<li>4</li>\n",
       "\t<li>2</li>\n",
       "\t<li>7</li>\n",
       "\t<li>2</li>\n",
       "\t<li>5</li>\n",
       "\t<li>723</li>\n",
       "\t<li>36</li>\n",
       "\t<li>71</li>\n",
       "\t<li>43</li>\n",
       "\t<li>530</li>\n",
       "\t<li>476</li>\n",
       "\t<li>26</li>\n",
       "\t<li>400</li>\n",
       "\t<li>317</li>\n",
       "\t<li>46</li>\n",
       "\t<li>7</li>\n",
       "\t<li>4</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>13</li>\n",
       "\t<li>104</li>\n",
       "\t<li>88</li>\n",
       "\t<li>4</li>\n",
       "\t<li>381</li>\n",
       "\t<li>15</li>\n",
       "\t<li>297</li>\n",
       "\t<li>98</li>\n",
       "\t<li>32</li>\n",
       "\t<li>2</li>\n",
       "\t<li>56</li>\n",
       "\t<li>26</li>\n",
       "\t<li>141</li>\n",
       "\t<li>6</li>\n",
       "\t<li>194</li>\n",
       "\t<li>2</li>\n",
       "\t<li>18</li>\n",
       "\t<li>4</li>\n",
       "\t<li>226</li>\n",
       "\t<li>22</li>\n",
       "\t<li>21</li>\n",
       "\t<li>134</li>\n",
       "\t<li>476</li>\n",
       "\t<li>26</li>\n",
       "\t<li>480</li>\n",
       "\t<li>5</li>\n",
       "\t<li>144</li>\n",
       "\t<li>30</li>\n",
       "\t<li>2</li>\n",
       "\t<li>18</li>\n",
       "\t<li>51</li>\n",
       "\t<li>36</li>\n",
       "\t<li>28</li>\n",
       "\t<li>224</li>\n",
       "\t<li>92</li>\n",
       "\t<li>25</li>\n",
       "\t<li>104</li>\n",
       "\t<li>4</li>\n",
       "\t<li>226</li>\n",
       "\t<li>65</li>\n",
       "\t<li>16</li>\n",
       "\t<li>38</li>\n",
       "\t<li>2</li>\n",
       "\t<li>88</li>\n",
       "\t<li>12</li>\n",
       "\t<li>16</li>\n",
       "\t<li>283</li>\n",
       "\t<li>5</li>\n",
       "\t<li>16</li>\n",
       "\t<li>2</li>\n",
       "\t<li>113</li>\n",
       "\t<li>103</li>\n",
       "\t<li>32</li>\n",
       "\t<li>15</li>\n",
       "\t<li>16</li>\n",
       "\t<li>2</li>\n",
       "\t<li>19</li>\n",
       "\t<li>178</li>\n",
       "\t<li>32</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 15\n",
       "\\item 256\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 7\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 723\n",
       "\\item 36\n",
       "\\item 71\n",
       "\\item 43\n",
       "\\item 530\n",
       "\\item 476\n",
       "\\item 26\n",
       "\\item 400\n",
       "\\item 317\n",
       "\\item 46\n",
       "\\item 7\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 13\n",
       "\\item 104\n",
       "\\item 88\n",
       "\\item 4\n",
       "\\item 381\n",
       "\\item 15\n",
       "\\item 297\n",
       "\\item 98\n",
       "\\item 32\n",
       "\\item 2\n",
       "\\item 56\n",
       "\\item 26\n",
       "\\item 141\n",
       "\\item 6\n",
       "\\item 194\n",
       "\\item 2\n",
       "\\item 18\n",
       "\\item 4\n",
       "\\item 226\n",
       "\\item 22\n",
       "\\item 21\n",
       "\\item 134\n",
       "\\item 476\n",
       "\\item 26\n",
       "\\item 480\n",
       "\\item 5\n",
       "\\item 144\n",
       "\\item 30\n",
       "\\item 2\n",
       "\\item 18\n",
       "\\item 51\n",
       "\\item 36\n",
       "\\item 28\n",
       "\\item 224\n",
       "\\item 92\n",
       "\\item 25\n",
       "\\item 104\n",
       "\\item 4\n",
       "\\item 226\n",
       "\\item 65\n",
       "\\item 16\n",
       "\\item 38\n",
       "\\item 2\n",
       "\\item 88\n",
       "\\item 12\n",
       "\\item 16\n",
       "\\item 283\n",
       "\\item 5\n",
       "\\item 16\n",
       "\\item 2\n",
       "\\item 113\n",
       "\\item 103\n",
       "\\item 32\n",
       "\\item 15\n",
       "\\item 16\n",
       "\\item 2\n",
       "\\item 19\n",
       "\\item 178\n",
       "\\item 32\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 15\n",
       "2. 256\n",
       "3. 4\n",
       "4. 2\n",
       "5. 7\n",
       "6. 2\n",
       "7. 5\n",
       "8. 723\n",
       "9. 36\n",
       "10. 71\n",
       "11. 43\n",
       "12. 530\n",
       "13. 476\n",
       "14. 26\n",
       "15. 400\n",
       "16. 317\n",
       "17. 46\n",
       "18. 7\n",
       "19. 4\n",
       "20. 2\n",
       "21. 2\n",
       "22. 13\n",
       "23. 104\n",
       "24. 88\n",
       "25. 4\n",
       "26. 381\n",
       "27. 15\n",
       "28. 297\n",
       "29. 98\n",
       "30. 32\n",
       "31. 2\n",
       "32. 56\n",
       "33. 26\n",
       "34. 141\n",
       "35. 6\n",
       "36. 194\n",
       "37. 2\n",
       "38. 18\n",
       "39. 4\n",
       "40. 226\n",
       "41. 22\n",
       "42. 21\n",
       "43. 134\n",
       "44. 476\n",
       "45. 26\n",
       "46. 480\n",
       "47. 5\n",
       "48. 144\n",
       "49. 30\n",
       "50. 2\n",
       "51. 18\n",
       "52. 51\n",
       "53. 36\n",
       "54. 28\n",
       "55. 224\n",
       "56. 92\n",
       "57. 25\n",
       "58. 104\n",
       "59. 4\n",
       "60. 226\n",
       "61. 65\n",
       "62. 16\n",
       "63. 38\n",
       "64. 2\n",
       "65. 88\n",
       "66. 12\n",
       "67. 16\n",
       "68. 283\n",
       "69. 5\n",
       "70. 16\n",
       "71. 2\n",
       "72. 113\n",
       "73. 103\n",
       "74. 32\n",
       "75. 15\n",
       "76. 16\n",
       "77. 2\n",
       "78. 19\n",
       "79. 178\n",
       "80. 32\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1]  15 256   4   2   7   2   5 723  36  71  43 530 476  26 400 317  46   7   4\n",
       "[20]   2   2  13 104  88   4 381  15 297  98  32   2  56  26 141   6 194   2  18\n",
       "[39]   4 226  22  21 134 476  26 480   5 144  30   2  18  51  36  28 224  92  25\n",
       "[58] 104   4 226  65  16  38   2  88  12  16 283   5  16   2 113 103  32  15  16\n",
       "[77]   2  19 178  32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_x[1,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create our neural network for sentiment classification and view its summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Layer (type)                        Output Shape                    Param #     \n",
      "================================================================================\n",
      "embedding (Embedding)               (None, None, 128)               256000      \n",
      "________________________________________________________________________________\n",
      "simple_rnn (SimpleRNN)              (None, 32)                      5152        \n",
      "________________________________________________________________________________\n",
      "dense (Dense)                       (None, 1)                       33          \n",
      "================================================================================\n",
      "Total params: 261,185\n",
      "Trainable params: 261,185\n",
      "Non-trainable params: 0\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model <- keras_model_sequential()\n",
    "model %>%\n",
    "  layer_embedding(input_dim = 1000, output_dim = 128) %>% \n",
    "  layer_simple_rnn(units = 32) %>% \n",
    "  layer_dense(units = 1, activation = 'sigmoid')\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us compile the model created above and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model %>% compile(\n",
    "  loss = 'binary_crossentropy',\n",
    "  optimizer = 'adam',\n",
    "  metrics = c('accuracy')\n",
    ")\n",
    "\n",
    "# train model\n",
    "model %>% fit(\n",
    "  train_x,train_y,\n",
    "  batch_size = 32,\n",
    "  epochs = 10,\n",
    "  validation_split = .2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model_hdf5(model,\"simple_rnn.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us evaluate the model and print out its test scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.8564493 \n",
      "Test accuracy 0.71648"
     ]
    }
   ],
   "source": [
    "scores <- model %>% evaluate(\n",
    "  test_x, test_y,\n",
    "  batch_size = 32\n",
    ")\n",
    "\n",
    "cat('Test score:', scores[[1]],'\\n')\n",
    "cat('Test accuracy', scores[[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1:\n",
    "\n",
    "In this example, we are using the imdb reviews built in dataset from keras library. In the first step, we are loading the training and testing datasets. The data has been mapped to a specific sequence of integer values, each integer representing a particular word in a dictionary. This dictionary has a rich collection of words arranged based on the frequency of each word getting used in the corpus. You can see that the dictionary is a list of key-value pairs, keys representing the words and values representing the index of the word in the dictionary. To discard the words which are not frequently used, we give a threshold of 1000,i.e. we will keep only the top 1000 most frequent words in our training dataset and ignore the rest.\n",
    "\n",
    "Step 2:\n",
    "\n",
    "In this step we are showcasing how to regenerate the reviews.\n",
    "\n",
    "Step 3:\n",
    "\n",
    "In this step we prepare the data to feed into the model. Since we cannot directly pass a list of integers into the model, we convert them into uniformly shaped tensors. To make the length of all the reviews uniform, we can follow either of these two approaches:\n",
    "\n",
    "1. One hot encoding of all the reviews to convert them into tensors of same length. The size of the matrix will be - \"number of words  *  number of reviews\".This approach is computationally heavy.\n",
    "\n",
    "2. Padding the reviews - Alternatively, we can pad all the reviews, so they all have the same length. This will create an integer tensor of shape  \"num_examples  *  max_length\".  The max_length argument is used to cap the maximum number of words that we want to keep in all the reviews.\n",
    "\n",
    "Since the second approach is less memory and computationally intensive, we will go for the second approach,i.e. padding the sequences.\n",
    "\n",
    "Step 4:\n",
    "\n",
    "In the next step, we define keras sequential model and configure the layers. The first layer is the embedding layer that is used to generate context out of our word sequences from our data and give information about relevant features. In an embedding, the words are represented by dense vector representations where a vector represents the projection of the word into a continuous vector space which is learnt from the text and is based on the words that surround a particular word. This position of the word in the vector space is referred to as its embedding. When we do embedding, we represent each review in terms of some latent factors.For example the word \"brilliant\" can be represented by a vector ,let's say- [.32, .02, .48, .21, .56, .15]. This is computationally efficient when using massive datasets since it reduces the dimensionality. The embedded vectors also get updated during the training process of the deep neural network, which helps in identifying similar words in a multi-dimensional space. Word embeddings also reflect how words are related to each other semantically. For example, words like \"talking\" and \"talked\" can be thought of as related in the same way as \"swimming\" is related to \"swam\".\n",
    "\n",
    "\n",
    "\n",
    "The Embedding layer is defined by specifying 3 arguments:\n",
    "- input_dim: This is the size of the vocabulary in the text data. In our example, the text data is integer encoded to values between 0-9999, then the size of the vocabulary would be 1000 words.\n",
    "- output_dim: This is the size of the vector space in which words will be embedded. We have specified it as 128.\n",
    "- input_length: This is the length of input sequences, as we define for any input layer of a keras model. This argument is required if we are going to connect Flatten then Dense layers upstream.\n",
    "In the next layer, we define a simple RNN model with 32 hidden units. If  is the number of input dimensions and  is the number of hidden units in RNN layer, then the number of trainable parameters is given by:\n",
    "\n",
    "\n",
    "The last layer is densely connected with a single output node, and here we use the sigmoid activation function since this is a binary classification task.\n",
    "\n",
    "Step 5\n",
    "\n",
    "Next, we compile the model. We specify \"binary_crossentropy\" as the loss function since we are dealing with binary classification here, and this loss function is preferred for models that output probabilities. The optimizer used is \"adam\". We then fit our training data into the model.\n",
    "\n",
    "Step 6\n",
    "\n",
    "In this step, we evaluate the test accuracy of our model to see how is our model performing on test data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There is more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, you are aware of how backpropagation through time(BPTT) works in an RNN. We traverse the network backwards, calculating gradients of errors with respect to the weights in each iteration. As we move closer to the early layers of the network, these gradients become too small, thus making the neurons in these layers learn very slowly.\n",
    "For an accurate model, it is crucial for the early layers to get trained accurately since these layers are responsible to learn simple patterns from the input and pass the relevant information accordingly to the following layers.\n",
    "RNNs often face this challenge when we train huge networks with more dependencies within the layers.\n",
    "This challenge is referred to as the vanishing gradient problem which makes the network learn too slowly and also the results not so accurate.\n",
    "It is often advised to use the RELU activation function to avoid vanishing gradient problem in large networks.\n",
    "Another very common way to deal with this issue is to use LSTMs(long short term memory) model about which we will talk in the following recipe.\n",
    "\n",
    "Another challenge that RNNs encounter is exploding gradient problem.\n",
    "In this case, we can see large gradients values, which in turn make the model learn too fast and inaccurately. In some cases, gradients can also become NaN due to numerical overflow in computations.\n",
    "We can see that the weights in the network increase by huge margins within less time while training.\n",
    "The most commonly used remedy for preventing this problem is by applying gradient clipping which prevents\n",
    "the gradients to increase beyond a specified threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer_repeat_vector()\n",
    "time_distributed() timeseries_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See also..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
