{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting ready..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(keras)\n",
    "library(readr)\n",
    "library(stringr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read our data and look at its sturcture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "data <- read_file(\"data/rhyme.txt\") %>% str_to_lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'jack and jill went up the hill\\nto fetch a pail of water.\\njack fell down and broke his crown,\\nand jill came tumbling after.\\n\\nup jack got and home did trot\\nas fast as he could caper;\\nand went to bed to mend his head\\nwith vinegar and brown paper.\\n'"
      ],
      "text/latex": [
       "'jack and jill went up the hill\\textbackslash{}nto fetch a pail of water.\\textbackslash{}njack fell down and broke his crown,\\textbackslash{}nand jill came tumbling after.\\textbackslash{}n\\textbackslash{}nup jack got and home did trot\\textbackslash{}nas fast as he could caper;\\textbackslash{}nand went to bed to mend his head\\textbackslash{}nwith vinegar and brown paper.\\textbackslash{}n'"
      ],
      "text/markdown": [
       "'jack and jill went up the hill\\nto fetch a pail of water.\\njack fell down and broke his crown,\\nand jill came tumbling after.\\n\\nup jack got and home did trot\\nas fast as he could caper;\\nand went to bed to mend his head\\nwith vinegar and brown paper.\\n'"
      ],
      "text/plain": [
       "[1] \"jack and jill went up the hill\\nto fetch a pail of water.\\njack fell down and broke his crown,\\nand jill came tumbling after.\\n\\nup jack got and home did trot\\nas fast as he could caper;\\nand went to bed to mend his head\\nwith vinegar and brown paper.\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = as.array(data)\n",
    "tokenizer = text_tokenizer(num_words = 40,char_level = F)\n",
    "tokenizer %>% fit_text_tokenizer(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saveRDS(object = tokenizer,\"token_nietzsche.rds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl>\n",
       "\t<dt>$and</dt>\n",
       "\t\t<dd>1</dd>\n",
       "\t<dt>$jack</dt>\n",
       "\t\t<dd>2</dd>\n",
       "\t<dt>$to</dt>\n",
       "\t\t<dd>3</dd>\n",
       "\t<dt>$jill</dt>\n",
       "\t\t<dd>4</dd>\n",
       "\t<dt>$went</dt>\n",
       "\t\t<dd>5</dd>\n",
       "\t<dt>$up</dt>\n",
       "\t\t<dd>6</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description}\n",
       "\\item[\\$and] 1\n",
       "\\item[\\$jack] 2\n",
       "\\item[\\$to] 3\n",
       "\\item[\\$jill] 4\n",
       "\\item[\\$went] 5\n",
       "\\item[\\$up] 6\n",
       "\\end{description}\n"
      ],
      "text/markdown": [
       "$and\n",
       ":   1\n",
       "$jack\n",
       ":   2\n",
       "$to\n",
       ":   3\n",
       "$jill\n",
       ":   4\n",
       "$went\n",
       ":   5\n",
       "$up\n",
       ":   6\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "$and\n",
       "[1] 1\n",
       "\n",
       "$jack\n",
       "[1] 2\n",
       "\n",
       "$to\n",
       "[1] 3\n",
       "\n",
       "$jill\n",
       "[1] 4\n",
       "\n",
       "$went\n",
       "[1] 5\n",
       "\n",
       "$up\n",
       "[1] 6\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(tokenizer$word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_seqs <- texts_to_sequences(tokenizer, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of 1\n",
      " $ : int [1:51] 2 1 4 5 6 9 10 3 11 12 ...\n"
     ]
    }
   ],
   "source": [
    "str(text_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_seqs <- text_seqs[[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_seqs <- text_seqs[1:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "51"
      ],
      "text/latex": [
       "51"
      ],
      "text/markdown": [
       "51"
      ],
      "text/plain": [
       "[1] 51"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "length(text_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>4</li>\n",
       "\t<li>5</li>\n",
       "\t<li>6</li>\n",
       "\t<li>9</li>\n",
       "\t<li>10</li>\n",
       "\t<li>3</li>\n",
       "\t<li>11</li>\n",
       "\t<li>12</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 4\n",
       "\\item 5\n",
       "\\item 6\n",
       "\\item 9\n",
       "\\item 10\n",
       "\\item 3\n",
       "\\item 11\n",
       "\\item 12\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 2\n",
       "2. 1\n",
       "3. 4\n",
       "4. 5\n",
       "5. 6\n",
       "6. 9\n",
       "7. 10\n",
       "8. 3\n",
       "9. 11\n",
       "10. 12\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1]  2  1  4  5  6  9 10  3 11 12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_seqs[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length_of_sentence <- 2\n",
    "feature <- matrix(ncol = train_length_of_sentence)\n",
    "label <- matrix(ncol = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature <- readRDS(\"feature_71.rds\")\n",
    "# label <- readRDS(\"label_71.rds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "for(i in seq(train_length_of_sentence, length(text_seqs))){\n",
    "    if(i >= length(text_seqs)){\n",
    "        break()\n",
    "    }\n",
    "    start_idx <- (i - train_length_of_sentence) +1\n",
    "    end_idx <- i +1\n",
    "    new_seq <-  text_seqs[start_idx:end_idx]\n",
    "    feature <- rbind(feature,new_seq[1:train_length_of_sentence])\n",
    "    label <- rbind(label,new_seq[train_length_of_sentence+1])\n",
    "}\n",
    "feature <- feature[-1,]\n",
    "label <- label[-1,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td>NA</td><td>NA</td></tr>\n",
       "\t<tr><td> 2</td><td> 1</td></tr>\n",
       "\t<tr><td> 1</td><td> 4</td></tr>\n",
       "\t<tr><td> 4</td><td> 5</td></tr>\n",
       "\t<tr><td> 5</td><td> 6</td></tr>\n",
       "\t<tr><td> 6</td><td> 9</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{ll}\n",
       "\t NA & NA\\\\\n",
       "\t  2 &  1\\\\\n",
       "\t  1 &  4\\\\\n",
       "\t  4 &  5\\\\\n",
       "\t  5 &  6\\\\\n",
       "\t  6 &  9\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| NA | NA |\n",
       "|  2 |  1 |\n",
       "|  1 |  4 |\n",
       "|  4 |  5 |\n",
       "|  5 |  6 |\n",
       "|  6 |  9 |\n",
       "\n"
      ],
      "text/plain": [
       "     [,1] [,2]\n",
       "[1,] NA   NA  \n",
       "[2,]  2    1  \n",
       "[3,]  1    4  \n",
       "[4,]  4    5  \n",
       "[5,]  5    6  \n",
       "[6,]  6    9  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td>NA</td></tr>\n",
       "\t<tr><td> 4</td></tr>\n",
       "\t<tr><td> 5</td></tr>\n",
       "\t<tr><td> 6</td></tr>\n",
       "\t<tr><td> 9</td></tr>\n",
       "\t<tr><td>10</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{l}\n",
       "\t NA\\\\\n",
       "\t  4\\\\\n",
       "\t  5\\\\\n",
       "\t  6\\\\\n",
       "\t  9\\\\\n",
       "\t 10\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| NA |\n",
       "|  4 |\n",
       "|  5 |\n",
       "|  6 |\n",
       "|  9 |\n",
       "| 10 |\n",
       "\n"
      ],
      "text/plain": [
       "     [,1]\n",
       "[1,] NA  \n",
       "[2,]  4  \n",
       "[3,]  5  \n",
       "[4,]  6  \n",
       "[5,]  9  \n",
       "[6,] 10  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>49</li>\n",
       "\t<li>2</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 49\n",
       "\\item 2\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 49\n",
       "2. 2\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 49  2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(feature)\n",
    "\n",
    "length(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "label <- to_categorical(label,num_classes = tokenizer$num_words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saveRDS(object = feature,file = \"feature_71.rds\")\n",
    "# saveRDS(label,file = \"label_71.rds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_index <- sample(1:nrow(feature), 0.8 * nrow(feature))\n",
    "# test_index <- setdiff(1:nrow(feature), train_index)\n",
    "# X_train <- feature[train_index,]\n",
    "# y_train <- label[train_index]\n",
    "# X_test <- feature[test_index,]\n",
    "# y_test <- label[test_index]\n",
    "\n",
    "# X_train <- to_categorical(X_train,num_classes = tokenizer$num_words )\n",
    "# y_train <- to_categorical(y_train,num_classes = tokenizer$num_words )\n",
    "\n",
    "# X_test <- to_categorical(X_test,num_classes = tokenizer$num_words )\n",
    "# y_test <- to_categorical(y_test,num_classes = tokenizer$num_words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of features 49 2 \n",
      "Shape of label 1960"
     ]
    }
   ],
   "source": [
    "cat(\"Shape of features\",dim(feature),\"\\n\")\n",
    "cat(\"Shape of label\",length(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "model <- keras_model_sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "model %>%\n",
    "    layer_embedding(input_dim = tokenizer$num_words,output_dim = 10,input_length = train_length_of_sentence) %>%\n",
    "    layer_lstm(units = 50) %>%\n",
    "    layer_dense(tokenizer$num_words) %>%\n",
    "    layer_activation(\"softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Layer (type)                        Output Shape                    Param #     \n",
      "================================================================================\n",
      "embedding_5 (Embedding)             (None, 2, 10)                   400         \n",
      "________________________________________________________________________________\n",
      "lstm_5 (LSTM)                       (None, 50)                      12200       \n",
      "________________________________________________________________________________\n",
      "dense_5 (Dense)                     (None, 40)                      2040        \n",
      "________________________________________________________________________________\n",
      "activation_5 (Activation)           (None, 40)                      0           \n",
      "================================================================================\n",
      "Total params: 14,640\n",
      "Trainable params: 14,640\n",
      "Non-trainable params: 0\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "model %>% compile(\n",
    "    loss = \"categorical_crossentropy\", \n",
    "    optimizer = optimizer_rmsprop(lr = 0.001),\n",
    "    metrics = c('accuracy')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "model %>% fit(\n",
    "  feature, label,\n",
    "  batch_size = 128,\n",
    "  epochs = 500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model_hdf5(model,\"lstm_v2.h5\")\n",
    "# library(keras)\n",
    "# model <- load_model_hdf5(filepath = \"lstm_v2.h5\")\n",
    "# summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores <- model %>% evaluate(\n",
    "#   X_test, y_test,\n",
    "#   batch_size = 32\n",
    "# )\n",
    "\n",
    "# cat('Test score:', scores[[1]],'\\n')\n",
    "# cat('Test accuracy', scores[[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "generate_sequence <-function(model, tokenizer, max_length, seed_text, n_words){\n",
    "    input_text <- seed_text\n",
    "    for(i in seq(n_words)){\n",
    "        encoded <- texts_to_sequences(tokenizer,input_text)[[1]]\n",
    "        encoded<- pad_sequences(sequences = list(encoded),maxlen = max_length,padding = 'pre')\n",
    "        yhat <- predict_classes(model,encoded, verbose=0)\n",
    "        next_word <- tokenizer$index_word[[as.character(yhat)]]\n",
    "        input_text <- paste(input_text, next_word)\n",
    "    }\n",
    "    return(input_text)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = \"Jack and\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'Jack and jill went up the hill to fetch a pail of water'"
      ],
      "text/latex": [
       "'Jack and jill went up the hill to fetch a pail of water'"
      ],
      "text/markdown": [
       "'Jack and jill went up the hill to fetch a pail of water'"
      ],
      "text/plain": [
       "[1] \"Jack and jill went up the hill to fetch a pail of water\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_sequence(model,tokenizer,train_length_of_sentence,seed,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_2 = \"Jack fell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'Jack fell down and broke his crown and jill went up the hill'"
      ],
      "text/latex": [
       "'Jack fell down and broke his crown and jill went up the hill'"
      ],
      "text/markdown": [
       "'Jack fell down and broke his crown and jill went up the hill'"
      ],
      "text/plain": [
       "[1] \"Jack fell down and broke his crown and jill went up the hill\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_sequence(model,tokenizer,train_length_of_sentence,seed_2,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_csv(\"data/ArticlesFeb2018.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the headlines column is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data$headline <- as.character(data$headline)\n",
    "headlines <- data$headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "headlines[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = as.array(headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text_tokenizer(num_words = 2000,char_level = F)\n",
    "tokenizer %>% fit_text_tokenizer(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(tokenizer$index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_seqs <- texts_to_sequences(tokenizer, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_seqs[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate mapping of index to words\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_of_headlines = sapply(headlines,function(x){sapply(strsplit(x, \" \"), length)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "headlines[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(x = length_of_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the sequences of tokens\n",
    "train_length_of_sentence <- 5\n",
    "feature <- matrix(ncol = train_length_of_sentence)\n",
    "label <- matrix(ncol = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for(headline in text_seqs){\n",
    "    for(i in seq(train_length_of_sentence, length(headline))){\n",
    "        if(i >= length(headline)){\n",
    "            break()\n",
    "        }\n",
    "        start_idx <- (i - train_length_of_sentence) +1\n",
    "        end_idx <- i +1\n",
    "        new_seq <-  headline[start_idx:end_idx]\n",
    "        feature <- rbind(feature,new_seq[1:train_length_of_sentence])\n",
    "        label <- rbind(label,new_seq[train_length_of_sentence+1])\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature <- feature[-1,]\n",
    "label <- label[-1,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index <- sample(1:nrow(feature), 0.8 * nrow(feature))\n",
    "test_index <- setdiff(1:nrow(feature), train_index)\n",
    "X_train <- feature[train_index,]\n",
    "y_train <- label[train_index]\n",
    "X_test <- feature[test_index,]\n",
    "y_test <- label[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train <- to_categorical(X_train,num_classes = tokenizer$num_words )\n",
    "y_train <- to_categorical(y_train,num_classes = tokenizer$num_words )\n",
    "X_test <- to_categorical(X_test,num_classes = tokenizer$num_words )\n",
    "y_test <- to_categorical(y_test,num_classes = tokenizer$num_words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat(\"Shape of features\",dim(X_train),\"\\n\")\n",
    "cat(\"Shape of features\",dim(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model <- keras_model_sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model %>%\n",
    "  layer_lstm(8, input_shape = c(train_length_of_sentence, tokenizer$num_words)) %>%\n",
    "  layer_dense(tokenizer$num_words) %>%\n",
    "  layer_activation(\"softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model %>% compile(\n",
    "    loss = \"categorical_crossentropy\", \n",
    "    optimizer = optimizer_rmsprop(lr = 0.001),\n",
    "    metrics = c('accuracy')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model %>% fit(\n",
    "  X_train, y_train,\n",
    "  batch_size = 128,\n",
    "  epochs = 15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores <- model %>% evaluate(\n",
    "  X_test, y_test,\n",
    "  batch_size = 32\n",
    ")\n",
    "\n",
    "cat('Test score:', scores[[1]],'\\n')\n",
    "cat('Test accuracy', scores[[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There is more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See also..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library(keras)\n",
    "# library(readr)\n",
    "# library(stringr)\n",
    "# library(purrr)\n",
    "# library(tokenizers)\n",
    "\n",
    "# # Parameters --------------------------------------------------------------\n",
    "\n",
    "# maxlen <- 40\n",
    "\n",
    "# # Data Preparation --------------------------------------------------------\n",
    "\n",
    "# # Retrieve text\n",
    "# path <- get_file(\n",
    "#   'nietzsche.txt', \n",
    "#   origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt'\n",
    "#   )\n",
    "\n",
    "# # Load, collapse, and tokenize text\n",
    "# text <- read_lines(path) %>%\n",
    "#   str_to_lower() %>%\n",
    "#   str_c(collapse = \"\\n\") %>%\n",
    "#   tokenize_characters(strip_non_alphanum = FALSE, simplify = TRUE)\n",
    "\n",
    "# print(sprintf(\"corpus length: %d\", length(text)))\n",
    "\n",
    "# text <- text[1:10000]\n",
    "\n",
    "# head(text)\n",
    "\n",
    "# chars <- text %>%\n",
    "#   unique() %>%\n",
    "#   sort()\n",
    "\n",
    "# print(sprintf(\"total chars: %d\", length(chars)))  \n",
    "\n",
    "# chars\n",
    "\n",
    "# # Cut the text in semi-redundant sequences of maxlen characters\n",
    "# dataset <- map(\n",
    "#   seq(1, length(text) - maxlen - 1, by = 3), \n",
    "#   ~list(sentece = text[.x:(.x + maxlen - 1)], next_char = text[.x + maxlen])\n",
    "#   )\n",
    "\n",
    "# dataset <- transpose(dataset)\n",
    "\n",
    "# head(dataset[[1]])\n",
    "\n",
    "# str(dataset)\n",
    "\n",
    "# length(dataset$sentece[[1]])\n",
    "\n",
    "# length(dataset$sentece)\n",
    "\n",
    "# 3320*40\n",
    "\n",
    "# # Vectorization\n",
    "# x <- array(0, dim = c(length(dataset$sentece), maxlen, length(chars)))\n",
    "# y <- array(0, dim = c(length(dataset$sentece), length(chars)))\n",
    "\n",
    "# for(i in 1:length(dataset$sentece)){\n",
    "  \n",
    "#   x[i,,] <- sapply(chars, function(x){\n",
    "#     as.integer(x == dataset$sentece[[i]])\n",
    "#   })\n",
    "  \n",
    "#   y[i,] <- as.integer(chars == dataset$next_char[[i]])\n",
    "  \n",
    "# }\n",
    "\n",
    "# dim(x)\n",
    "\n",
    "# dim(y)\n",
    "\n",
    "# x[1,,]\n",
    "\n",
    "# x[1,2,]\n",
    "\n",
    "# class(x[1,2,])\n",
    "\n",
    "# length(x[1,2,])\n",
    "\n",
    "# y[1,]\n",
    "\n",
    "# length(y[1,])\n",
    "\n",
    "# # Model Definition --------------------------------------------------------\n",
    "\n",
    "# model <- keras_model_sequential()\n",
    "\n",
    "# model %>%\n",
    "#   layer_lstm(128, input_shape = c(maxlen, length(chars))) %>%\n",
    "#   layer_dense(length(chars)) %>%\n",
    "#   layer_activation(\"softmax\")\n",
    "\n",
    "# optimizer <- optimizer_rmsprop(lr = 0.01)\n",
    "\n",
    "# model %>% compile(\n",
    "#   loss = \"categorical_crossentropy\", \n",
    "#   optimizer = optimizer\n",
    "# )\n",
    "\n",
    "# # Training & Results ----------------------------------------------------\n",
    "\n",
    "# sample_mod <- function(preds, temperature = 1){\n",
    "#   preds <- log(preds)/temperature\n",
    "#   exp_preds <- exp(preds)\n",
    "#   preds <- exp_preds/sum(exp(preds))\n",
    "  \n",
    "#   rmultinom(1, 1, preds) %>% \n",
    "#     as.integer() %>%\n",
    "#     which.max()\n",
    "# }\n",
    "\n",
    "# on_epoch_end <- function(epoch, logs) {\n",
    "  \n",
    "#   cat(sprintf(\"epoch: %02d ---------------\\n\\n\", epoch))\n",
    "  \n",
    "#   for(diversity in c(0.2, 0.5, 1, 1.2)){\n",
    "    \n",
    "#     cat(sprintf(\"diversity: %f ---------------\\n\\n\", diversity))\n",
    "    \n",
    "#     start_index <- sample(1:(length(text) - maxlen), size = 1)\n",
    "#     sentence <- text[start_index:(start_index + maxlen - 1)]\n",
    "#     generated <- \"\"\n",
    "    \n",
    "#     for(i in 1:400){\n",
    "      \n",
    "#       x <- sapply(chars, function(x){\n",
    "#         as.integer(x == sentence)\n",
    "#       })\n",
    "#       x <- array_reshape(x, c(1, dim(x)))\n",
    "      \n",
    "#       preds <- predict(model, x)\n",
    "#       next_index <- sample_mod(preds, diversity)\n",
    "#       next_char <- chars[next_index]\n",
    "      \n",
    "#       generated <- str_c(generated, next_char, collapse = \"\")\n",
    "#       sentence <- c(sentence[-1], next_char)\n",
    "      \n",
    "#     }\n",
    "    \n",
    "#     cat(generated)\n",
    "#     cat(\"\\n\\n\")\n",
    "    \n",
    "#   }\n",
    "# }\n",
    "\n",
    "# print_callback <- callback_lambda(on_epoch_end = on_epoch_end)\n",
    "\n",
    "# model %>% fit(\n",
    "#   x, y,\n",
    "#   batch_size = 128,\n",
    "#   epochs = 1,\n",
    "#   callbacks = print_callback\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will be working with -------- data set.It conta.........\n",
    "We will start by importing the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library(keras)\n",
    "# library(readr)\n",
    "# library(stringr)\n",
    "# # library(tm)\n",
    "# # library(tokenizers)\n",
    "\n",
    "# # Let's read our data and look at its sturcture\n",
    "\n",
    "# data <- read_file(\"data/nietzsche_71.txt\") %>% str_to_lower()\n",
    "\n",
    "# data <- str_split(data, \" \")\n",
    "\n",
    "# str(data)\n",
    "\n",
    "# data <- unlist(data)\n",
    "\n",
    "# str(data)\n",
    "\n",
    "# length(data)\n",
    "\n",
    "# data <- data[1:100]\n",
    "\n",
    "# train_length_of_sentence <- 20\n",
    "# feature <- matrix(ncol = 1)\n",
    "# label <- matrix(ncol = 1)\n",
    "\n",
    "# for(i in seq(train_length_of_sentence, length(data))){\n",
    "#     if(i >= length(data)){\n",
    "#         break()\n",
    "#     }\n",
    "#     start_idx <- (i - train_length_of_sentence) +1\n",
    "#     end_idx <- i +1\n",
    "#     new_seq <-  data[start_idx:end_idx]\n",
    "#     feature <- rbind(feature,paste(new_seq[1:train_length_of_sentence],collapse = \" \"))\n",
    "#     label <- rbind(label,new_seq[train_length_of_sentence+1])\n",
    "# }\n",
    "\n",
    "# new_seq[train_length_of_sentence+1]\n",
    "\n",
    "# # feature <- readRDS(\"feature_71.rds\")\n",
    "\n",
    "# # label <- readRDS(\"label_71.rds\")\n",
    "\n",
    "# head(feature)\n",
    "\n",
    "# head(label)\n",
    "\n",
    "# # feature <- feature[-1,]\n",
    "# # label <- label[-1,]\n",
    "\n",
    "# dim(feature)\n",
    "\n",
    "# length(label)\n",
    "\n",
    "# # saveRDS(object = feature,file = \"feature_71.rds\")\n",
    "\n",
    "# # saveRDS(label,file = \"label_71.rds\")\n",
    "\n",
    "# train_index <- sample(1:nrow(feature), 0.8 * nrow(feature))\n",
    "# test_index <- setdiff(1:nrow(feature), train_index)\n",
    "# X_train <- feature[train_index,]\n",
    "# y_train <- label[train_index]\n",
    "# X_test <- feature[test_index,]\n",
    "# y_test <- label[test_index]\n",
    "\n",
    "# str(data)\n",
    "\n",
    "# tokenizer = text_tokenizer(num_words = 2000,char_level = F)\n",
    "# tokenizer %>% fit_text_tokenizer(data)\n",
    "\n",
    "# head(tokenizer$index_word)\n",
    "\n",
    "# # train_seq_gen <- texts_to_sequences_generator(tokenizer,texts = c(X_train,y_train))\n",
    "\n",
    "# # train_seq_gen_y <- texts_to_sequences_generator(tokenizer,y_train)\n",
    "\n",
    "# X_train[1:2]\n",
    "\n",
    "# y_train[1:2]\n",
    "\n",
    "# texts_to_sequences(tokenizer,X_train[1:2,])\n",
    "\n",
    "# sampling_generator <- function(X_data, Y_data = NULL, batch_size = 32) {\n",
    "#     function() {\n",
    "#       gc() # should blow up R if we are ever called on a background thread\n",
    "#       rows <- sample(1:nrow(X_data), batch_size, replace = TRUE)\n",
    "#       if (!is.null(Y_data))\n",
    "#         list(texts_to_sequences(X_data[rows]), texts_to_sequences(Y_data[rows]))\n",
    "#       else\n",
    "#         list(texts_to_sequences(X_data[rows,]))\n",
    "#     }\n",
    "#   }\n",
    "\n",
    "# model <- keras_model_sequential()\n",
    "\n",
    "# model %>%\n",
    "#   layer_lstm(8, input_shape = c(train_length_of_sentence, tokenizer$num_words)) %>%\n",
    "#   layer_dense(tokenizer$num_words) %>%\n",
    "#   layer_activation(\"softmax\")\n",
    "\n",
    "# summary(model)\n",
    "\n",
    "# model %>% compile(\n",
    "#     loss = \"categorical_crossentropy\", \n",
    "#     optimizer = optimizer_rmsprop(lr = 0.001),\n",
    "#     metrics = c('accuracy')\n",
    "# )\n",
    "\n",
    "\n",
    "# model %>% fit_generator(generator = sampling_generator(X_data = X_train,Y_data = y_train,batch_size = 20),steps_per_epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
