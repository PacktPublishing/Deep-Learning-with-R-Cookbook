{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "library(reinforcelearn)\n",
    "library(ReinforcementLearning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CliffWalking>\n",
       "  Inherits from: <Gridworld>\n",
       "  Public:\n",
       "    action.names: 0 1 2 3\n",
       "    action.space: Discrete\n",
       "    actions: 0 1 2 3\n",
       "    clone: function (deep = FALSE) \n",
       "    discount: 1\n",
       "    done: FALSE\n",
       "    episode: 0\n",
       "    episode.return: 0\n",
       "    episode.step: 0\n",
       "    initial.state: 36\n",
       "    initialize: function (...) \n",
       "    n.actions: 4\n",
       "    n.states: 48\n",
       "    n.step: 0\n",
       "    previous.state: NULL\n",
       "    reset: function () \n",
       "    resetEverything: function () \n",
       "    reward: NULL\n",
       "    rewards: -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...\n",
       "    state: 36\n",
       "    state.space: Discrete\n",
       "    states: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  ...\n",
       "    step: function (action) \n",
       "    terminal.states: 47\n",
       "    transitions: 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  ...\n",
       "    visualize: function () \n",
       "  Private:\n",
       "    reset_: function (env) \n",
       "    step_: function (env, action) \n",
       "    visualize_: function (env) "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loading the cliff walking environment\n",
    "env = makeEnvironment(\"cliff.walking\")\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>State</dt>\n",
       "\t\t<dd>'character'</dd>\n",
       "\t<dt>Action</dt>\n",
       "\t\t<dd>'character'</dd>\n",
       "\t<dt>Reward</dt>\n",
       "\t\t<dd>'numeric'</dd>\n",
       "\t<dt>NextState</dt>\n",
       "\t\t<dd>'character'</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[State] 'character'\n",
       "\\item[Action] 'character'\n",
       "\\item[Reward] 'numeric'\n",
       "\\item[NextState] 'character'\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "State\n",
       ":   'character'Action\n",
       ":   'character'Reward\n",
       ":   'numeric'NextState\n",
       ":   'character'\n",
       "\n"
      ],
      "text/plain": [
       "      State      Action      Reward   NextState \n",
       "\"character\" \"character\"   \"numeric\" \"character\" "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>State</th><th scope=col>Action</th><th scope=col>Reward</th><th scope=col>NextState</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>36  </td><td>1   </td><td>  -1</td><td>36  </td></tr>\n",
       "\t<tr><td>36  </td><td>3   </td><td>  -1</td><td>36  </td></tr>\n",
       "\t<tr><td>36  </td><td>0   </td><td>  -1</td><td>36  </td></tr>\n",
       "\t<tr><td>36  </td><td>0   </td><td>  -1</td><td>36  </td></tr>\n",
       "\t<tr><td>36  </td><td>2   </td><td>  -1</td><td>12  </td></tr>\n",
       "\t<tr><td>12  </td><td>3   </td><td>  -1</td><td>36  </td></tr>\n",
       "\t<tr><td>36  </td><td>2   </td><td>  -1</td><td>12  </td></tr>\n",
       "\t<tr><td>12  </td><td>3   </td><td>  -1</td><td>36  </td></tr>\n",
       "\t<tr><td>36  </td><td>2   </td><td>  -1</td><td>12  </td></tr>\n",
       "\t<tr><td>12  </td><td>1   </td><td>  -1</td><td>14  </td></tr>\n",
       "\t<tr><td>14  </td><td>3   </td><td>  -1</td><td>38  </td></tr>\n",
       "\t<tr><td>38  </td><td>3   </td><td>-100</td><td>36  </td></tr>\n",
       "\t<tr><td>36  </td><td>1   </td><td>  -1</td><td>36  </td></tr>\n",
       "\t<tr><td>36  </td><td>1   </td><td>  -1</td><td>36  </td></tr>\n",
       "\t<tr><td>36  </td><td>3   </td><td>  -1</td><td>36  </td></tr>\n",
       "\t<tr><td>36  </td><td>1   </td><td>  -1</td><td>36  </td></tr>\n",
       "\t<tr><td>36  </td><td>1   </td><td>  -1</td><td>36  </td></tr>\n",
       "\t<tr><td>36  </td><td>0   </td><td>  -1</td><td>36  </td></tr>\n",
       "\t<tr><td>36  </td><td>2   </td><td>  -1</td><td>12  </td></tr>\n",
       "\t<tr><td>12  </td><td>2   </td><td>  -1</td><td>0   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " State & Action & Reward & NextState\\\\\n",
       "\\hline\n",
       "\t 36   & 1    &   -1 & 36  \\\\\n",
       "\t 36   & 3    &   -1 & 36  \\\\\n",
       "\t 36   & 0    &   -1 & 36  \\\\\n",
       "\t 36   & 0    &   -1 & 36  \\\\\n",
       "\t 36   & 2    &   -1 & 12  \\\\\n",
       "\t 12   & 3    &   -1 & 36  \\\\\n",
       "\t 36   & 2    &   -1 & 12  \\\\\n",
       "\t 12   & 3    &   -1 & 36  \\\\\n",
       "\t 36   & 2    &   -1 & 12  \\\\\n",
       "\t 12   & 1    &   -1 & 14  \\\\\n",
       "\t 14   & 3    &   -1 & 38  \\\\\n",
       "\t 38   & 3    & -100 & 36  \\\\\n",
       "\t 36   & 1    &   -1 & 36  \\\\\n",
       "\t 36   & 1    &   -1 & 36  \\\\\n",
       "\t 36   & 3    &   -1 & 36  \\\\\n",
       "\t 36   & 1    &   -1 & 36  \\\\\n",
       "\t 36   & 1    &   -1 & 36  \\\\\n",
       "\t 36   & 0    &   -1 & 36  \\\\\n",
       "\t 36   & 2    &   -1 & 12  \\\\\n",
       "\t 12   & 2    &   -1 & 0   \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| State | Action | Reward | NextState |\n",
       "|---|---|---|---|\n",
       "| 36   | 1    |   -1 | 36   |\n",
       "| 36   | 3    |   -1 | 36   |\n",
       "| 36   | 0    |   -1 | 36   |\n",
       "| 36   | 0    |   -1 | 36   |\n",
       "| 36   | 2    |   -1 | 12   |\n",
       "| 12   | 3    |   -1 | 36   |\n",
       "| 36   | 2    |   -1 | 12   |\n",
       "| 12   | 3    |   -1 | 36   |\n",
       "| 36   | 2    |   -1 | 12   |\n",
       "| 12   | 1    |   -1 | 14   |\n",
       "| 14   | 3    |   -1 | 38   |\n",
       "| 38   | 3    | -100 | 36   |\n",
       "| 36   | 1    |   -1 | 36   |\n",
       "| 36   | 1    |   -1 | 36   |\n",
       "| 36   | 3    |   -1 | 36   |\n",
       "| 36   | 1    |   -1 | 36   |\n",
       "| 36   | 1    |   -1 | 36   |\n",
       "| 36   | 0    |   -1 | 36   |\n",
       "| 36   | 2    |   -1 | 12   |\n",
       "| 12   | 2    |   -1 | 0    |\n",
       "\n"
      ],
      "text/plain": [
       "   State Action Reward NextState\n",
       "1  36    1        -1   36       \n",
       "2  36    3        -1   36       \n",
       "3  36    0        -1   36       \n",
       "4  36    0        -1   36       \n",
       "5  36    2        -1   12       \n",
       "6  12    3        -1   36       \n",
       "7  36    2        -1   12       \n",
       "8  12    3        -1   36       \n",
       "9  36    2        -1   12       \n",
       "10 12    1        -1   14       \n",
       "11 14    3        -1   38       \n",
       "12 38    3      -100   36       \n",
       "13 36    1        -1   36       \n",
       "14 36    1        -1   36       \n",
       "15 36    3        -1   36       \n",
       "16 36    1        -1   36       \n",
       "17 36    1        -1   36       \n",
       "18 36    0        -1   36       \n",
       "19 36    2        -1   12       \n",
       "20 12    2        -1   0        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# creating the function to query the environment\n",
    "sequences <- function(iter,env){\n",
    "    actions <- env$actions\n",
    "    data <- data.frame(matrix(ncol = 4, nrow = 0))\n",
    "    colnames(data) <- c(\"State\", \"Action\", \"Reward\",\"NextState\")\n",
    "    env$reset()\n",
    "    for(i in 1:iter){\n",
    "    current_state <- env$state\n",
    "    current_action <- floor(runif(1,0,4))\n",
    "    current_reward <- env$step(current_action)$reward\n",
    "    next_state_iter <- env$step(current_action)$state\n",
    "    iter_data <- cbind(\"State\" = current_state,\"Action\" = current_action,\"Reward\"=current_reward,\"NextState\" = next_state_iter)\n",
    "    data <- rbind(data,iter_data)\n",
    "    if(env$done == \"TRUE\"){\n",
    "        break;\n",
    "    }\n",
    "    }\n",
    "   return(data) \n",
    "}\n",
    "\n",
    "# getting the data from function\n",
    "\n",
    "iter <- 1000\n",
    "observations = sequences(iter,env)  \n",
    "\n",
    "cols.name <- c(\"State\",\"Action\",\"NextState\")\n",
    "observations[cols.name] <- sapply(observations[cols.name],as.character)\n",
    "sapply(observations, class)\n",
    "\n",
    "# displaying first 20 records\n",
    "head(observations,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing reinforcement learning\n",
    "\n",
    "control <- list(alpha = 0.2, gamma = 0.4, epsilon = 0.1)\n",
    "\n",
    "model <- ReinforcementLearning(data = observations, s = \"State\", a = \"Action\", r = \"Reward\",\n",
    "                               s_new = \"NextState\", iter = 1, control = control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-Action function Q\n",
      "             0           1           2           3\n",
      "X36 -1.5953639  -1.5952462  -1.4948993  -1.5949936\n",
      "X37  0.0000000   0.0000000 -20.0000000   0.0000000\n",
      "X38  0.0000000 -20.0000000 -36.1893567 -49.0059376\n",
      "X24 -1.4648553  -1.3324273  -1.4812546  -1.4917518\n",
      "X26 -1.3897756  -0.9741327  -1.3121494  -1.5148258\n",
      "X28 -0.8636740  -0.8293248  -0.7959076  -1.3823701\n",
      "X0  -1.5642927  -1.4662367  -1.5676150  -1.5102539\n",
      "X2  -1.5380473  -1.2780173  -1.4251813  -1.3349818\n",
      "X12 -1.4644123  -1.2590694  -1.5630369  -1.5862153\n",
      "X30 -0.9595646  -0.2000000  -0.2288000  -0.5727028\n",
      "X4  -1.3953639  -0.9776434  -1.3279273  -0.9610399\n",
      "X14 -1.2198423  -0.8657823  -1.1166604  -0.7378560\n",
      "X32  0.0000000   0.0000000   0.0000000  -0.3169141\n",
      "X6  -1.0338219  -0.5904000  -0.7425920  -0.6064000\n",
      "X16 -0.7560023  -0.3600000  -0.4278656   0.0000000\n",
      "X8  -0.6791040   0.0000000  -0.6723200   0.0000000\n",
      "X18 -0.2000000   0.0000000  -0.2288000   0.0000000\n",
      "\n",
      "Policy\n",
      "X36 X37 X38 X24 X26 X28  X0  X2 X12 X30  X4 X14 X32  X6 X16  X8 X18 \n",
      "\"2\" \"0\" \"0\" \"1\" \"1\" \"2\" \"1\" \"1\" \"1\" \"1\" \"3\" \"3\" \"0\" \"1\" \"3\" \"1\" \"1\" \n",
      "\n",
      "Reward (last iteration)\n",
      "[1] -1693\n"
     ]
    }
   ],
   "source": [
    "# printing the learnt state-action table which contains the Q-value of each state-action pair\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There is more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>State</dt>\n",
       "\t\t<dd>'character'</dd>\n",
       "\t<dt>Action</dt>\n",
       "\t\t<dd>'character'</dd>\n",
       "\t<dt>Reward</dt>\n",
       "\t\t<dd>'numeric'</dd>\n",
       "\t<dt>NextState</dt>\n",
       "\t\t<dd>'character'</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[State] 'character'\n",
       "\\item[Action] 'character'\n",
       "\\item[Reward] 'numeric'\n",
       "\\item[NextState] 'character'\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "State\n",
       ":   'character'Action\n",
       ":   'character'Reward\n",
       ":   'numeric'NextState\n",
       ":   'character'\n",
       "\n"
      ],
      "text/plain": [
       "      State      Action      Reward   NextState \n",
       "\"character\" \"character\"   \"numeric\" \"character\" "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>State</th><th scope=col>Action</th><th scope=col>Reward</th><th scope=col>NextState</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>36</td><td>2 </td><td>-1</td><td>12</td></tr>\n",
       "\t<tr><td>12</td><td>2 </td><td>-1</td><td>0 </td></tr>\n",
       "\t<tr><td>0 </td><td>1 </td><td>-1</td><td>2 </td></tr>\n",
       "\t<tr><td>2 </td><td>2 </td><td>-1</td><td>2 </td></tr>\n",
       "\t<tr><td>2 </td><td>0 </td><td>-1</td><td>0 </td></tr>\n",
       "\t<tr><td>0 </td><td>2 </td><td>-1</td><td>0 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " State & Action & Reward & NextState\\\\\n",
       "\\hline\n",
       "\t 36 & 2  & -1 & 12\\\\\n",
       "\t 12 & 2  & -1 & 0 \\\\\n",
       "\t 0  & 1  & -1 & 2 \\\\\n",
       "\t 2  & 2  & -1 & 2 \\\\\n",
       "\t 2  & 0  & -1 & 0 \\\\\n",
       "\t 0  & 2  & -1 & 0 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| State | Action | Reward | NextState |\n",
       "|---|---|---|---|\n",
       "| 36 | 2  | -1 | 12 |\n",
       "| 12 | 2  | -1 | 0  |\n",
       "| 0  | 1  | -1 | 2  |\n",
       "| 2  | 2  | -1 | 2  |\n",
       "| 2  | 0  | -1 | 0  |\n",
       "| 0  | 2  | -1 | 0  |\n",
       "\n"
      ],
      "text/plain": [
       "  State Action Reward NextState\n",
       "1 36    2      -1     12       \n",
       "2 12    2      -1     0        \n",
       "3 0     1      -1     2        \n",
       "4 2     2      -1     2        \n",
       "5 2     0      -1     0        \n",
       "6 0     2      -1     0        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# getting 100 new sample data from the cliff walking environment\n",
    "new_observations =  sequences(100,env) \n",
    "cols.name <- c(\"State\",\"Action\",\"NextState\")\n",
    "new_observations[cols.name] <- sapply(new_observations[cols.name],as.character)\n",
    "sapply(new_observations, class)\n",
    "head(new_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-Action function Q\n",
      "            X0          X1          X2         X3\n",
      "24  -1.1096681  -1.0985934  -1.1113893 -1.1124119\n",
      "26  -1.0924338  -1.0325305  -1.1120072 -1.1123957\n",
      "28  -0.9840692  -0.7796008  -1.1051685 -1.0663833\n",
      "29   0.0000000   0.0000000   0.0000000 -0.7325490\n",
      "30  -0.5651880  -0.8773772   0.0000000 -1.0066913\n",
      "0   -1.1109654  -1.1101782  -1.1109736 -1.1097649\n",
      "32  -0.8393290  -0.5748472  -0.8657922 -0.6671089\n",
      "2   -1.1129358  -1.1059532  -1.1101509 -1.1022497\n",
      "10  -0.2995810  -0.2972000   0.0000000 -0.6556262\n",
      "33   0.0000000  -0.6556262   0.0000000  0.0000000\n",
      "11  -0.6556262  -1.0067198  -1.0423613 -0.7520509\n",
      "34  -0.5449394  -0.6900636   0.0000000  0.0000000\n",
      "4   -1.1084350  -1.0618690  -1.1059733 -1.0732534\n",
      "12  -1.1073482  -1.0750293  -1.1110217 -1.1108705\n",
      "35  -0.6556262   0.0000000  -0.9214708 -0.3520000\n",
      "5    0.0000000  -0.6556262   0.0000000 -0.6556262\n",
      "6   -1.0792958  -0.9430099  -0.9793951 -0.6938900\n",
      "14  -1.1012670  -1.0530209  -1.1042439 -0.7551120\n",
      "36  -1.1107382  -1.1107402  -1.1074477 -1.1107363\n",
      "37   0.0000000 -77.6845798 -66.1043617  0.0000000\n",
      "7   -0.8814067   0.0000000   0.0000000  0.0000000\n",
      "38 -28.1128567   0.0000000 -66.0891457  0.0000000\n",
      "8   -0.3107360  -0.8214766  -0.7964146 -0.9617286\n",
      "16  -0.8003083  -0.6900636  -0.9445578 -0.8828708\n",
      "9   -0.6556262   0.0000000   0.0000000  0.0000000\n",
      "18  -0.6850296   0.0000000  -0.1065988  0.0000000\n",
      "40 -65.6268120 -65.9877760   0.0000000  0.0000000\n",
      "\n",
      "Policy\n",
      "  24   26   28   29   30    0   32    2   10   33   11   34    4   12   35    5 \n",
      "\"X1\" \"X1\" \"X1\" \"X0\" \"X2\" \"X3\" \"X1\" \"X3\" \"X2\" \"X0\" \"X0\" \"X2\" \"X1\" \"X1\" \"X1\" \"X0\" \n",
      "   6   14   36   37    7   38    8   16    9   18   40 \n",
      "\"X3\" \"X3\" \"X2\" \"X0\" \"X1\" \"X1\" \"X0\" \"X1\" \"X1\" \"X1\" \"X2\" \n",
      "\n",
      "Reward (last iteration)\n",
      "[1] -95\n"
     ]
    }
   ],
   "source": [
    "# providing our existing RL model as an argument to update the existing policy\n",
    "newmodel <- ReinforcementLearning(new_observations, \n",
    "                                   s = \"State\", \n",
    "                                   a = \"Action\", \n",
    "                                   r = \"Reward\", \n",
    "                                   s_new = \"NextState\", \n",
    "                                   model = model)\n",
    "print(newmodel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
