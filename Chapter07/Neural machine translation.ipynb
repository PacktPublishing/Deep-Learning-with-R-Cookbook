{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered S3 methods overwritten by 'ggplot2':\n",
      "  method         from \n",
      "  [.quosures     rlang\n",
      "  c.quosures     rlang\n",
      "  print.quosures rlang\n"
     ]
    }
   ],
   "source": [
    "# loading the required libraries\n",
    "library(keras)\n",
    "library(stringr)\n",
    "library(reshape2)\n",
    "library(purrr)\n",
    "library(ggplot2)\n",
    "library(readr)\n",
    "library(stringi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol>\n",
       "\t<li><ol class=list-inline>\n",
       "\t<li>'Hi.'</li>\n",
       "\t<li>'Hallo!'</li>\n",
       "</ol>\n",
       "</li>\n",
       "\t<li><ol class=list-inline>\n",
       "\t<li>'Hi.'</li>\n",
       "\t<li>'GrÃ¼ÃŸ Gott!'</li>\n",
       "</ol>\n",
       "</li>\n",
       "\t<li><ol class=list-inline>\n",
       "\t<li>'Run!'</li>\n",
       "\t<li>'Lauf!'</li>\n",
       "</ol>\n",
       "</li>\n",
       "\t<li><ol class=list-inline>\n",
       "\t<li>'Wow!'</li>\n",
       "\t<li>'Potzdonner!'</li>\n",
       "</ol>\n",
       "</li>\n",
       "\t<li><ol class=list-inline>\n",
       "\t<li>'Wow!'</li>\n",
       "\t<li>'Donnerwetter!'</li>\n",
       "</ol>\n",
       "</li>\n",
       "\t<li><ol class=list-inline>\n",
       "\t<li>'Fire!'</li>\n",
       "\t<li>'Feuer!'</li>\n",
       "</ol>\n",
       "</li>\n",
       "\t<li><ol class=list-inline>\n",
       "\t<li>'Help!'</li>\n",
       "\t<li>'Hilfe!'</li>\n",
       "</ol>\n",
       "</li>\n",
       "\t<li><ol class=list-inline>\n",
       "\t<li>'Help!'</li>\n",
       "\t<li>'Zu HÃ¼lf!'</li>\n",
       "</ol>\n",
       "</li>\n",
       "\t<li><ol class=list-inline>\n",
       "\t<li>'Stop!'</li>\n",
       "\t<li>'Stopp!'</li>\n",
       "</ol>\n",
       "</li>\n",
       "\t<li><ol class=list-inline>\n",
       "\t<li>'Wait!'</li>\n",
       "\t<li>'Warte!'</li>\n",
       "</ol>\n",
       "</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate}\n",
       "\\item \\begin{enumerate*}\n",
       "\\item 'Hi.'\n",
       "\\item 'Hallo!'\n",
       "\\end{enumerate*}\n",
       "\n",
       "\\item \\begin{enumerate*}\n",
       "\\item 'Hi.'\n",
       "\\item 'GrÃ¼ÃŸ Gott!'\n",
       "\\end{enumerate*}\n",
       "\n",
       "\\item \\begin{enumerate*}\n",
       "\\item 'Run!'\n",
       "\\item 'Lauf!'\n",
       "\\end{enumerate*}\n",
       "\n",
       "\\item \\begin{enumerate*}\n",
       "\\item 'Wow!'\n",
       "\\item 'Potzdonner!'\n",
       "\\end{enumerate*}\n",
       "\n",
       "\\item \\begin{enumerate*}\n",
       "\\item 'Wow!'\n",
       "\\item 'Donnerwetter!'\n",
       "\\end{enumerate*}\n",
       "\n",
       "\\item \\begin{enumerate*}\n",
       "\\item 'Fire!'\n",
       "\\item 'Feuer!'\n",
       "\\end{enumerate*}\n",
       "\n",
       "\\item \\begin{enumerate*}\n",
       "\\item 'Help!'\n",
       "\\item 'Hilfe!'\n",
       "\\end{enumerate*}\n",
       "\n",
       "\\item \\begin{enumerate*}\n",
       "\\item 'Help!'\n",
       "\\item 'Zu HÃ¼lf!'\n",
       "\\end{enumerate*}\n",
       "\n",
       "\\item \\begin{enumerate*}\n",
       "\\item 'Stop!'\n",
       "\\item 'Stopp!'\n",
       "\\end{enumerate*}\n",
       "\n",
       "\\item \\begin{enumerate*}\n",
       "\\item 'Wait!'\n",
       "\\item 'Warte!'\n",
       "\\end{enumerate*}\n",
       "\n",
       "\\end{enumerate}\n"
      ],
      "text/markdown": [
       "1. 1. 'Hi.'\n",
       "2. 'Hallo!'\n",
       "\n",
       "\n",
       "\n",
       "2. 1. 'Hi.'\n",
       "2. 'GrÃ¼ÃŸ Gott!'\n",
       "\n",
       "\n",
       "\n",
       "3. 1. 'Run!'\n",
       "2. 'Lauf!'\n",
       "\n",
       "\n",
       "\n",
       "4. 1. 'Wow!'\n",
       "2. 'Potzdonner!'\n",
       "\n",
       "\n",
       "\n",
       "5. 1. 'Wow!'\n",
       "2. 'Donnerwetter!'\n",
       "\n",
       "\n",
       "\n",
       "6. 1. 'Fire!'\n",
       "2. 'Feuer!'\n",
       "\n",
       "\n",
       "\n",
       "7. 1. 'Help!'\n",
       "2. 'Hilfe!'\n",
       "\n",
       "\n",
       "\n",
       "8. 1. 'Help!'\n",
       "2. 'Zu HÃ¼lf!'\n",
       "\n",
       "\n",
       "\n",
       "9. 1. 'Stop!'\n",
       "2. 'Stopp!'\n",
       "\n",
       "\n",
       "\n",
       "10. 1. 'Wait!'\n",
       "2. 'Warte!'\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[[1]]\n",
       "[1] \"Hi.\"    \"Hallo!\"\n",
       "\n",
       "[[2]]\n",
       "[1] \"Hi.\"          \"GrÃ¼ÃŸ Gott!\"\n",
       "\n",
       "[[3]]\n",
       "[1] \"Run!\"  \"Lauf!\"\n",
       "\n",
       "[[4]]\n",
       "[1] \"Wow!\"        \"Potzdonner!\"\n",
       "\n",
       "[[5]]\n",
       "[1] \"Wow!\"          \"Donnerwetter!\"\n",
       "\n",
       "[[6]]\n",
       "[1] \"Fire!\"  \"Feuer!\"\n",
       "\n",
       "[[7]]\n",
       "[1] \"Help!\"  \"Hilfe!\"\n",
       "\n",
       "[[8]]\n",
       "[1] \"Help!\"     \"Zu HÃ¼lf!\"\n",
       "\n",
       "[[9]]\n",
       "[1] \"Stop!\"  \"Stopp!\"\n",
       "\n",
       "[[10]]\n",
       "[1] \"Wait!\"  \"Warte!\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loading the first 10000 phrases from the data;this will be our input data\n",
    "lines <- readLines(\"data/deu.txt\", n = 10000)\n",
    "sentences <- str_split(lines, \"\\t\")\n",
    "sentences[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  cleaning the input data \n",
    "data_cleaning <- function(sentence) {\n",
    "  sentence = gsub('[[:punct:] ]+',' ',sentence)\n",
    "  sentence = gsub(\"[^[:alnum:]\\\\-\\\\.\\\\s]\", \" \", sentence)\n",
    "  sentence = stringi::stri_trans_general(sentence, \"latin-ascii\")\n",
    "  sentence = tolower(sentence)\n",
    "#   sentence = paste0(\"<start> \", sentence, \" <stop>\")\n",
    "  sentence\n",
    "}\n",
    "\n",
    "\n",
    "sentences <- map(sentences,data_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capturing the maximum length of statements in English and German \n",
    "english_sentences = list()\n",
    "german_sentences = list()\n",
    "\n",
    "for(i in 1:length(sentences)){\n",
    "    current_sentence <- sentences[i]%>%unlist()%>%str_split('\\t')\n",
    "    english_sentences <- append(english_sentences,current_sentence[1])\n",
    "    german_sentences <- append(german_sentences,current_sentence[2])  \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>German</th><th scope=col>English</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>hallo        </td><td>hi           </td></tr>\n",
       "\t<tr><td>gra a  gott  </td><td>hi           </td></tr>\n",
       "\t<tr><td>lauf         </td><td>run          </td></tr>\n",
       "\t<tr><td>potzdonner   </td><td>wow          </td></tr>\n",
       "\t<tr><td>donnerwetter </td><td>wow          </td></tr>\n",
       "\t<tr><td>feuer        </td><td>fire         </td></tr>\n",
       "\t<tr><td>hilfe        </td><td>help         </td></tr>\n",
       "\t<tr><td>zu ha lf     </td><td>help         </td></tr>\n",
       "\t<tr><td>stopp        </td><td>stop         </td></tr>\n",
       "\t<tr><td>warte        </td><td>wait         </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " German & English\\\\\n",
       "\\hline\n",
       "\t hallo         & hi           \\\\\n",
       "\t gra a  gott   & hi           \\\\\n",
       "\t lauf          & run          \\\\\n",
       "\t potzdonner    & wow          \\\\\n",
       "\t donnerwetter  & wow          \\\\\n",
       "\t feuer         & fire         \\\\\n",
       "\t hilfe         & help         \\\\\n",
       "\t zu ha lf      & help         \\\\\n",
       "\t stopp         & stop         \\\\\n",
       "\t warte         & wait         \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| German | English |\n",
       "|---|---|\n",
       "| hallo         | hi            |\n",
       "| gra a  gott   | hi            |\n",
       "| lauf          | run           |\n",
       "| potzdonner    | wow           |\n",
       "| donnerwetter  | wow           |\n",
       "| feuer         | fire          |\n",
       "| hilfe         | help          |\n",
       "| zu ha lf      | help          |\n",
       "| stopp         | stop          |\n",
       "| warte         | wait          |\n",
       "\n"
      ],
      "text/plain": [
       "   German        English\n",
       "1  hallo         hi     \n",
       "2  gra a  gott   hi     \n",
       "3  lauf          run    \n",
       "4  potzdonner    wow    \n",
       "5  donnerwetter  wow    \n",
       "6  feuer         fire   \n",
       "7  hilfe         help   \n",
       "8  zu ha lf      help   \n",
       "9  stopp         stop   \n",
       "10 warte         wait   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# converting the data into a dataframe\n",
    "data <- do.call(rbind, Map(data.frame, \"German\"=german_sentences,\"English\"=english_sentences))\n",
    "head(data,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Maximum length of a sentence in German data:10\"\n",
      "[1] \"Maximum length of a sentence in English data:6\"\n"
     ]
    }
   ],
   "source": [
    "# checking the maximum number of words in all the sentences in German and English phrases\n",
    "german_length = max(sapply(strsplit(as.character(data[,\"German\"] ), \" \"), length))\n",
    "print(paste0(\"Maximum length of a sentence in German data:\",german_length))\n",
    "\n",
    "eng_length = max(sapply(strsplit(as.character(data[,\"English\"] ), \" \"), length))\n",
    "print(paste0(\"Maximum length of a sentence in English data:\",eng_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function for tokenization\n",
    "tokenization <- function(lines){\n",
    "    tokenizer = text_tokenizer()\n",
    "    tokenizer =  fit_text_tokenizer(tokenizer,lines)\n",
    "    return(tokenizer)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"German Vocabulary Size:3542\"\n"
     ]
    }
   ],
   "source": [
    "# preparing German tokenizer\n",
    "german_tokenizer = tokenization(data[,\"German\"])\n",
    "german_vocab_size = length(german_tokenizer$word_index)  + 1\n",
    "\n",
    "print(paste0('German Vocabulary Size:',german_vocab_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"English Vocabulary Size:2189\"\n"
     ]
    }
   ],
   "source": [
    "# preparing English tokenizer\n",
    "eng_tokenizer = tokenization(data[,\"English\"])\n",
    "eng_vocab_size = length(eng_tokenizer$word_index) + 1\n",
    "\n",
    "print(paste0('English Vocabulary Size:',eng_vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to encode and pad sequences\n",
    "encode_pad_sequences <- function(tokenizer, length, lines){\n",
    "    # Encoding text to integers\n",
    "    seq = texts_to_sequences(tokenizer,lines)\n",
    "    # Padding text to maximum length sentence\n",
    "    seq = pad_sequences(seq, maxlen=length, padding='post')\n",
    "    return(seq)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing the data into training and testing datasets \n",
    "train_data <- data[1:9000,]\n",
    "test_data <- data[9001:10000,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the training and testing data\n",
    "x_train <- encode_pad_sequences(german_tokenizer,german_length,train_data[,\"German\"])\n",
    "y_train <- encode_pad_sequences(eng_tokenizer,eng_length,train_data[,\"English\"])\n",
    "y_train <- to_categorical(y_train,num_classes = eng_vocab_size)\n",
    "\n",
    "x_test <- encode_pad_sequences(german_tokenizer,german_length,test_data[,\"German\"])\n",
    "y_test <- encode_pad_sequences(eng_tokenizer,eng_length,test_data[,\"English\"])\n",
    "y_test <- to_categorical(y_test,num_classes = eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining network parameters for model\n",
    "in_vocab = german_vocab_size\n",
    "out_vocab = eng_vocab_size\n",
    "in_timesteps = german_length\n",
    "out_timesteps = eng_length\n",
    "units = 512\n",
    "epochs = 70\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building model\n",
    "model <- keras_model_sequential()\n",
    "model %>%\n",
    "    layer_embedding(in_vocab,units, input_length=in_timesteps, mask_zero=TRUE) %>%\n",
    "    layer_lstm(units = units) %>%\n",
    "    layer_repeat_vector(out_timesteps)%>%\n",
    "    layer_lstm(units,return_sequences = TRUE)%>%\n",
    "    time_distributed(layer_dense(units = out_vocab, activation='softmax'))\n",
    "\n",
    "# summary of the model\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model %>% compile(optimizer = \"adam\",loss = 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining callbacks and checkpoints\n",
    "\n",
    "model_name <- \"model_nmt\"\n",
    "\n",
    "checkpoint_dir <- \"checkpoints_nmt\"\n",
    "dir.create(checkpoint_dir)\n",
    "filepath <- file.path(checkpoint_dir, paste0(model_name,\"weights.{epoch:02d}-{val_loss:.2f}.hdf5\",sep=\"\"))\n",
    "\n",
    "cp_callback <- list(callback_model_checkpoint(mode = \"min\",\n",
    " filepath = filepath,\n",
    " save_best_only = TRUE,\n",
    " verbose = 1))\n",
    "#  callback_early_stopping(patience = 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "model %>% fit(x_train,y_train,epochs = epochs,batch_size = batch_size,validation_split = 0.2,callbacks = cp_callback,verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting for test data\n",
    "predicted = model %>% predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a reversed list of key-value pair of the word index\n",
    "reverse_word_index <- function(tokenizer){\n",
    "    reverse_word_index <- names(tokenizer$word_index)\n",
    "    names(reverse_word_index) <- tokenizer$word_index\n",
    "    return(reverse_word_index)\n",
    "}\n",
    "\n",
    "german_reverse_word_index <- reverse_word_index(german_tokenizer)\n",
    "eng_reverse_word_index <- reverse_word_index(eng_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoding sample phrases from test data in German to English\n",
    "index_to_word <- function(data_sample,word_index_dict){\n",
    "    phrase = list()\n",
    "    for(i in 1:length(data_sample)){\n",
    "        index = data_sample[[i]]\n",
    "        word = word_index_dict[index] \n",
    "#         word = if(!is.null(word)) word else \"?\"\n",
    "        phrase = paste0(phrase,\" \",word)\n",
    " }\n",
    "    return(phrase)\n",
    "\n",
    "}\n",
    "\n",
    "cat(paste0(\"The german sample phrase is -->\",index_to_word(x_test[41,],german_reverse_word_index)))\n",
    "cat('\\n')\n",
    "cat(paste0(\"The actual translation in english is -->\",as.character(test_data[41,\"English\"])))\n",
    "cat('\\n')\n",
    "cat(paste0(\"The predicted translation in english is -->\",index_to_word(predicted[41,],eng_reverse_word_index)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
