{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification of iris dataset- Species using Single Layer Neural Network**<br>\n",
    "\n",
    "- Add Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'keras' was built under R version 3.5.3\""
     ]
    }
   ],
   "source": [
    "library(keras)\n",
    "library(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading iris dataset from the library datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>150</li>\n",
       "\t<li>5</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 150\n",
       "\\item 5\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 150\n",
       "2. 5\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 150   5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Sepal.Length</th><th scope=col>Sepal.Width</th><th scope=col>Petal.Length</th><th scope=col>Petal.Width</th><th scope=col>Species</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>5.1   </td><td>3.5   </td><td>1.4   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>4.9   </td><td>3.0   </td><td>1.4   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>4.7   </td><td>3.2   </td><td>1.3   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>4.6   </td><td>3.1   </td><td>1.5   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>5.0   </td><td>3.6   </td><td>1.4   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>5.4   </td><td>3.9   </td><td>1.7   </td><td>0.4   </td><td>setosa</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       " Sepal.Length & Sepal.Width & Petal.Length & Petal.Width & Species\\\\\n",
       "\\hline\n",
       "\t 5.1    & 3.5    & 1.4    & 0.2    & setosa\\\\\n",
       "\t 4.9    & 3.0    & 1.4    & 0.2    & setosa\\\\\n",
       "\t 4.7    & 3.2    & 1.3    & 0.2    & setosa\\\\\n",
       "\t 4.6    & 3.1    & 1.5    & 0.2    & setosa\\\\\n",
       "\t 5.0    & 3.6    & 1.4    & 0.2    & setosa\\\\\n",
       "\t 5.4    & 3.9    & 1.7    & 0.4    & setosa\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "Sepal.Length | Sepal.Width | Petal.Length | Petal.Width | Species | \n",
       "|---|---|---|---|---|---|\n",
       "| 5.1    | 3.5    | 1.4    | 0.2    | setosa | \n",
       "| 4.9    | 3.0    | 1.4    | 0.2    | setosa | \n",
       "| 4.7    | 3.2    | 1.3    | 0.2    | setosa | \n",
       "| 4.6    | 3.1    | 1.5    | 0.2    | setosa | \n",
       "| 5.0    | 3.6    | 1.4    | 0.2    | setosa | \n",
       "| 5.4    | 3.9    | 1.7    | 0.4    | setosa | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n",
       "1 5.1          3.5         1.4          0.2         setosa \n",
       "2 4.9          3.0         1.4          0.2         setosa \n",
       "3 4.7          3.2         1.3          0.2         setosa \n",
       "4 4.6          3.1         1.5          0.2         setosa \n",
       "5 5.0          3.6         1.4          0.2         setosa \n",
       "6 5.4          3.9         1.7          0.4         setosa "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data <- datasets::iris\n",
    "dim(data)\n",
    "head(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t150 obs. of  5 variables:\n",
      " $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n",
      " $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n",
      " $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n",
      " $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n",
      " $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n"
     ]
    }
   ],
   "source": [
    "# Structure of data\n",
    "str(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n",
       " Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n",
       " 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n",
       " Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n",
       " Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n",
       " 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n",
       " Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n",
       "       Species  \n",
       " setosa    :50  \n",
       " versicolor:50  \n",
       " virginica :50  \n",
       "                \n",
       "                \n",
       "                "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Statistical sumary of data\n",
    "summary(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrplot\n",
    "# corrplot(data[,1:4],method = \"circle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying required transformations in data\n",
    "To work with keras package we need to convert the data to an array or a matrix.\n",
    "The matrix data elements should be of the same basic type but here we have target values that are of factor type and hence we   need to change this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>0  </td></tr>\n",
       "\t<tr><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>0  </td></tr>\n",
       "\t<tr><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>0  </td></tr>\n",
       "\t<tr><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>0  </td></tr>\n",
       "\t<tr><td>5.0</td><td>3.6</td><td>1.4</td><td>0.2</td><td>0  </td></tr>\n",
       "\t<tr><td>5.4</td><td>3.9</td><td>1.7</td><td>0.4</td><td>0  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{lllll}\n",
       "\t 5.1 & 3.5 & 1.4 & 0.2 & 0  \\\\\n",
       "\t 4.9 & 3.0 & 1.4 & 0.2 & 0  \\\\\n",
       "\t 4.7 & 3.2 & 1.3 & 0.2 & 0  \\\\\n",
       "\t 4.6 & 3.1 & 1.5 & 0.2 & 0  \\\\\n",
       "\t 5.0 & 3.6 & 1.4 & 0.2 & 0  \\\\\n",
       "\t 5.4 & 3.9 & 1.7 & 0.4 & 0  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| 5.1 | 3.5 | 1.4 | 0.2 | 0   | \n",
       "| 4.9 | 3.0 | 1.4 | 0.2 | 0   | \n",
       "| 4.7 | 3.2 | 1.3 | 0.2 | 0   | \n",
       "| 4.6 | 3.1 | 1.5 | 0.2 | 0   | \n",
       "| 5.0 | 3.6 | 1.4 | 0.2 | 0   | \n",
       "| 5.4 | 3.9 | 1.7 | 0.4 | 0   | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     [,1] [,2] [,3] [,4] [,5]\n",
       "[1,] 5.1  3.5  1.4  0.2  0   \n",
       "[2,] 4.9  3.0  1.4  0.2  0   \n",
       "[3,] 4.7  3.2  1.3  0.2  0   \n",
       "[4,] 4.6  3.1  1.5  0.2  0   \n",
       "[5,] 5.0  3.6  1.4  0.2  0   \n",
       "[6,] 5.4  3.9  1.7  0.4  0   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data[,5] <- as.numeric(data[,5]) -1\n",
    "\n",
    "# Turn data into a matrix\n",
    "data <- as.matrix(data)\n",
    "\n",
    "# Setting dimnames of data to NULL\n",
    "dimnames(data) <- NULL\n",
    "head(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the data into training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1\n",
       "2. 2\n",
       "3. 1\n",
       "4. 1\n",
       "5. 1\n",
       "6. 2\n",
       "7. 1\n",
       "8. 2\n",
       "9. 1\n",
       "10. 1\n",
       "11. 1\n",
       "12. 2\n",
       "13. 2\n",
       "14. 2\n",
       "15. 1\n",
       "16. 1\n",
       "17. 1\n",
       "18. 1\n",
       "19. 1\n",
       "20. 2\n",
       "21. 1\n",
       "22. 1\n",
       "23. 1\n",
       "24. 2\n",
       "25. 1\n",
       "26. 2\n",
       "27. 1\n",
       "28. 1\n",
       "29. 1\n",
       "30. 1\n",
       "31. 1\n",
       "32. 1\n",
       "33. 2\n",
       "34. 2\n",
       "35. 1\n",
       "36. 2\n",
       "37. 1\n",
       "38. 1\n",
       "39. 1\n",
       "40. 1\n",
       "41. 1\n",
       "42. 2\n",
       "43. 1\n",
       "44. 2\n",
       "45. 2\n",
       "46. 2\n",
       "47. 2\n",
       "48. 1\n",
       "49. 1\n",
       "50. 1\n",
       "51. 1\n",
       "52. 1\n",
       "53. 1\n",
       "54. 1\n",
       "55. 2\n",
       "56. 2\n",
       "57. 2\n",
       "58. 1\n",
       "59. 2\n",
       "60. 2\n",
       "61. 2\n",
       "62. 1\n",
       "63. 1\n",
       "64. 1\n",
       "65. 1\n",
       "66. 1\n",
       "67. 1\n",
       "68. 1\n",
       "69. 2\n",
       "70. 2\n",
       "71. 1\n",
       "72. 1\n",
       "73. 1\n",
       "74. 1\n",
       "75. 1\n",
       "76. 2\n",
       "77. 2\n",
       "78. 1\n",
       "79. 1\n",
       "80. 1\n",
       "81. 2\n",
       "82. 1\n",
       "83. 1\n",
       "84. 2\n",
       "85. 1\n",
       "86. 2\n",
       "87. 1\n",
       "88. 1\n",
       "89. 1\n",
       "90. 2\n",
       "91. 2\n",
       "92. 2\n",
       "93. 2\n",
       "94. 1\n",
       "95. 1\n",
       "96. 2\n",
       "97. 1\n",
       "98. 1\n",
       "99. 1\n",
       "100. 1\n",
       "101. 1\n",
       "102. 1\n",
       "103. 2\n",
       "104. 1\n",
       "105. 1\n",
       "106. 1\n",
       "107. 1\n",
       "108. 2\n",
       "109. 2\n",
       "110. 2\n",
       "111. 1\n",
       "112. 1\n",
       "113. 1\n",
       "114. 1\n",
       "115. 1\n",
       "116. 1\n",
       "117. 2\n",
       "118. 1\n",
       "119. 2\n",
       "120. 2\n",
       "121. 1\n",
       "122. 1\n",
       "123. 2\n",
       "124. 1\n",
       "125. 2\n",
       "126. 1\n",
       "127. 1\n",
       "128. 2\n",
       "129. 1\n",
       "130. 1\n",
       "131. 2\n",
       "132. 2\n",
       "133. 2\n",
       "134. 1\n",
       "135. 1\n",
       "136. 1\n",
       "137. 1\n",
       "138. 1\n",
       "139. 2\n",
       "140. 1\n",
       "141. 1\n",
       "142. 2\n",
       "143. 1\n",
       "144. 2\n",
       "145. 2\n",
       "146. 1\n",
       "147. 1\n",
       "148. 2\n",
       "149. 1\n",
       "150. 1\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  [1] 1 2 1 1 1 2 1 2 1 1 1 2 2 2 1 1 1 1 1 2 1 1 1 2 1 2 1 1 1 1 1 1 2 2 1 2 1\n",
       " [38] 1 1 1 1 2 1 2 2 2 2 1 1 1 1 1 1 1 2 2 2 1 2 2 2 1 1 1 1 1 1 1 2 2 1 1 1 1\n",
       " [75] 1 2 2 1 1 1 2 1 1 2 1 2 1 1 1 2 2 2 2 1 1 2 1 1 1 1 1 1 2 1 1 1 1 2 2 2 1\n",
       "[112] 1 1 1 1 1 2 1 2 2 1 1 2 1 2 1 1 2 1 1 2 2 2 1 1 1 1 1 2 1 1 2 1 2 2 1 1 2\n",
       "[149] 1 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setting seed to avoid random values each time the code is run ;\n",
    "set.seed(76)\n",
    "# Training and testing data sample size\n",
    "indexes <- sample(2,nrow(data),replace = TRUE,prob = c(0.67,0.33))\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the `iris` data\n",
    "data.train <- data[indexes==1, 1:4]\n",
    "data.test <- data[indexes==2, 1:4]\n",
    "\n",
    "# Split the class attribute\n",
    "data.trainingtarget <- data[indexes==1, 5]\n",
    "data.testtarget <- data[indexes==2, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding for target variable\n",
    "While building a multi class classification model in neural networks it is recommended to \n",
    "transform your target attribute from a vector that contains values for each class value to a matrix \n",
    "with a boolean for each class value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2\n",
      "[39] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"
     ]
    }
   ],
   "source": [
    "# One hot encoding of training and test data target values\n",
    "data.trainLabels <- to_categorical(data.trainingtarget)\n",
    "\n",
    "# One hot encode test target values\n",
    "data.testLabels <- to_categorical(data.testtarget)\n",
    "\n",
    "# Print out the test labels to double check the result\n",
    "print(data.testtarget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a sequential model\n",
    "model <- keras_model_sequential() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add layer to the model\n",
    "model %>% \n",
    "    layer_dense(units = 3, activation = 'softmax',input_shape = ncol(data.train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add layers to the model\n",
    "# model %>% \n",
    "#     layer_dense(units = 8, activation = 'relu', input_shape = ncol(data.train)) %>% \n",
    "#     layer_dense(units = 3, activation = 'softmax')\n",
    "\n",
    "# The output layer creates 3 output values, one for each Iris class (versicolor, virginica or setosa). The first layer, which contains 8 hidden notes, on the other hand, has an input_shape of 4.\n",
    "# This is because your training data data.train has 4 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can further inspect your model with the following functions:\n",
    "\n",
    "summary() function  - print a summary representation of your model;<br>\n",
    "get_config() - return a list that contains the configuration of the model;<br>\n",
    "get_layer() - return the layer configuration.<br>\n",
    "layers attribute - used to retrieve a flattened list of the model’s layers;<br>\n",
    "\n",
    "To list the input tensors, you can use the inputs attribute; and <br>\n",
    "To retrieve the output tensors, you can make use of the outputs attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Layer (type)                        Output Shape                    Param #     \n",
      "================================================================================\n",
      "dense_10 (Dense)                    (None, 3)                       15          \n",
      "================================================================================\n",
      "Total params: 15\n",
      "Trainable params: 15\n",
      "Non-trainable params: 0\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'sequential_5', 'layers': [{'class_name': 'Dense', 'config': {'name': 'dense_10', 'trainable': True, 'batch_input_shape': (None, 4), 'dtype': 'float32', 'units': 3, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Dense>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[1]]\n",
       "<keras.layers.core.Dense>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[1]]\n",
       "Tensor(\"dense_10_input:0\", shape=(?, 4), dtype=float32)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[1]]\n",
       "Tensor(\"dense_10/Softmax:0\", shape=(?, 3), dtype=float32)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get model configuration\n",
    "get_config(model)\n",
    "\n",
    "# Get layer configuration\n",
    "get_layer(model, index = 1)\n",
    "\n",
    "# List the model's layers\n",
    "model$layers\n",
    "\n",
    "# List the input tensors\n",
    "model$inputs\n",
    "\n",
    "# List the output tensors\n",
    "model$outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile And Fit The Model\n",
    "\n",
    "Some common optimization algorithms used are the Stochastic Gradient Descent (SGD), ADAM and RMSprop.<br>\n",
    "Depending on whichever algorithm you choose, you’ll need to tune certain parameters, such as learning rate or momentum.<br>\n",
    "The choice for a loss function depends on the task that you have at hand: for example, for a classsification problem, you’ll usually use cross entropy and for a binary classification problem we use binary_crossentropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model %>% compile(\n",
    "     loss = 'categorical_crossentropy',\n",
    "     optimizer = 'adam',\n",
    "     metrics = 'accuracy'\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model \n",
    "model %>% fit(\n",
    "     data.train, \n",
    "     data.trainLabels, \n",
    "     epochs = 200, \n",
    "     batch_size = 5, \n",
    "     validation_split = 0.2\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAP1BMVEUAAAAAv8QzMzNNTU1o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\ngAgSRQFEkCgKoP8DgXexnoJracoAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history <- model %>% fit(\n",
    "     data.train, \n",
    "     data.trainLabels, \n",
    "     epochs = 200,\n",
    "     batch_size = 5, \n",
    "     validation_split = 0.2\n",
    " )\n",
    "\n",
    "# Plot the history\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss and acc indicate the loss and accuracy of the model for the training data, while the val_loss and val_acc are the same metrics, loss and accuracy, for the test or validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the classes for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               classes\n",
       "data.testtarget  0  1  2\n",
       "              0 17  0  0\n",
       "              1  0 18  0\n",
       "              2  0 13  5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes <- model %>% predict_classes(data.test, batch_size = 128)\n",
    "\n",
    "# Confusion matrix\n",
    "table(data.testtarget, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$loss\n",
      "[1] 0.3954425\n",
      "\n",
      "$acc\n",
      "[1] 0.754717\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test data and labels\n",
    "score <- model %>% evaluate(data.test, data.testLabels, batch_size = 128)\n",
    "\n",
    "# Print the score\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
