{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrplot\n",
    "# corrplot(data[,1:4],method = \"circle\")\n",
    "\n",
    "### Applying required transformations in data\n",
    "To work with keras package we need to convert the data to an array or a matrix.\n",
    "The matrix data elements should be of the same basic type but here we have target values that are of factor type and hence we   need to change this.\n",
    "\n",
    "\n",
    "\n",
    "data[,5] <- as.numeric(data[,5]) -1\n",
    "\n",
    "# Turn data into a matrix\n",
    "data <- as.matrix(data)\n",
    "\n",
    "# Setting dimnames of data to NULL\n",
    "dimnames(data) <- NULL\n",
    "head(data)\n",
    "\n",
    "### Dividing the data into training and testing datasets\n",
    "\n",
    "# Setting seed to avoid random values each time the code is run ;\n",
    "set.seed(76)\n",
    "# Training and testing data sample size\n",
    "indexes <- sample(2,nrow(data),replace = TRUE,prob = c(0.67,0.33))\n",
    "indexes\n",
    "\n",
    "# Splitting the `iris` data\n",
    "data.train <- data[indexes==1, 1:4]\n",
    "data.test <- data[indexes==2, 1:4]\n",
    "\n",
    "# Split the class attribute\n",
    "data.trainingtarget <- data[indexes==1, 5]\n",
    "data.testtarget <- data[indexes==2, 5]\n",
    "\n",
    "### One Hot Encoding for target variable\n",
    "While building a multi class classification model in neural networks it is recommended to \n",
    "transform your target attribute from a vector that contains values for each class value to a matrix \n",
    "with a boolean for each class value.\n",
    "\n",
    "# One hot encoding of training and test data target values\n",
    "data.trainLabels <- to_categorical(data.trainingtarget)\n",
    "\n",
    "# One hot encode test target values\n",
    "data.testLabels <- to_categorical(data.testtarget)\n",
    "\n",
    "# Print out the test labels to double check the result\n",
    "print(data.testtarget)\n",
    "\n",
    "### Building the model\n",
    "\n",
    "# Initialize a sequential model\n",
    "model <- keras_model_sequential() \n",
    "\n",
    "# Add layers to the model\n",
    "model %>% \n",
    "    layer_dense(units = 8, activation = 'relu', input_shape = ncol(data.train)) %>% \n",
    "    layer_dense(units = 5, activation = 'relu') %>% \n",
    "    layer_dense(units = 3, activation = 'softmax')\n",
    "\n",
    "# The output layer creates 3 output values, one for each Iris class (versicolor, virginica or setosa). The first layer, which contains 8 hidden notes, on the other hand, has an input_shape of 4.\n",
    "# This is because your training data data.train has 4 columns.\n",
    "\n",
    "You can further inspect your model with the following functions:\n",
    "\n",
    "summary() function  - print a summary representation of your model;<br>\n",
    "get_config() - return a list that contains the configuration of the model;<br>\n",
    "get_layer() - return the layer configuration.<br>\n",
    "layers attribute - used to retrieve a flattened list of the model’s layers;<br>\n",
    "\n",
    "To list the input tensors, you can use the inputs attribute; and <br>\n",
    "To retrieve the output tensors, you can make use of the outputs attribute\n",
    "\n",
    "summary(model)\n",
    "\n",
    "# Get model configuration\n",
    "get_config(model)\n",
    "\n",
    "# Get layer configuration\n",
    "get_layer(model, index = 1)\n",
    "\n",
    "# List the model's layers\n",
    "model$layers\n",
    "\n",
    "# List the input tensors\n",
    "model$inputs\n",
    "\n",
    "# List the output tensors\n",
    "model$outputs\n",
    "\n",
    "#### Compile And Fit The Model\n",
    "\n",
    "Some common optimization algorithms used are the Stochastic Gradient Descent (SGD), ADAM and RMSprop.<br>\n",
    "Depending on whichever algorithm you choose, you’ll need to tune certain parameters, such as learning rate or momentum.<br>\n",
    "The choice for a loss function depends on the task that you have at hand: for example, for a classsification problem, you’ll usually use cross entropy and for a binary classification problem we use binary_crossentropy loss function.\n",
    "\n",
    "# Compile the model\n",
    "model %>% compile(\n",
    "     loss = 'categorical_crossentropy',\n",
    "     optimizer = 'adam',\n",
    "     metrics = 'accuracy'\n",
    " )\n",
    "\n",
    "# Fit the model \n",
    "model %>% fit(\n",
    "     data.train, \n",
    "     data.trainLabels, \n",
    "     epochs = 200, \n",
    "     batch_size = 5, \n",
    "     validation_split = 0.2\n",
    " )\n",
    "\n",
    "#### Visualizing the model\n",
    "\n",
    "history <- model %>% fit(\n",
    "     data.train, \n",
    "     data.trainLabels, \n",
    "     epochs = 200,\n",
    "     batch_size = 5, \n",
    "     validation_split = 0.2\n",
    " )\n",
    "\n",
    "# Plot the history\n",
    "plot(history)\n",
    "\n",
    "loss and acc indicate the loss and accuracy of the model for the training data, while the val_loss and val_acc are the same metrics, loss and accuracy, for the test or validation data.\n",
    "\n",
    "### Predicting the classes for the test data\n",
    "\n",
    "classes <- model %>% predict_classes(data.test, batch_size = 128)\n",
    "\n",
    "# Confusion matrix\n",
    "table(data.testtarget, classes)\n",
    "\n",
    "### Evaluating the model\n",
    "\n",
    "# Evaluate on test data and labels\n",
    "score <- model %>% evaluate(data.test, data.testLabels, batch_size = 128)\n",
    "\n",
    "# Print the score\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
