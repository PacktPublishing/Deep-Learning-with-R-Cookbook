{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification of iris dataset- Species using Single Layer Neural Network**<br>\n",
    "\n",
    "- Add Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "library(keras)\n",
    "library(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading iris dataset from the library datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>150</li>\n",
       "\t<li>5</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 150\n",
       "\\item 5\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 150\n",
       "2. 5\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 150   5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Sepal.Length</th><th scope=col>Sepal.Width</th><th scope=col>Petal.Length</th><th scope=col>Petal.Width</th><th scope=col>Species</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>5.1   </td><td>3.5   </td><td>1.4   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>4.9   </td><td>3.0   </td><td>1.4   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>4.7   </td><td>3.2   </td><td>1.3   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>4.6   </td><td>3.1   </td><td>1.5   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>5.0   </td><td>3.6   </td><td>1.4   </td><td>0.2   </td><td>setosa</td></tr>\n",
       "\t<tr><td>5.4   </td><td>3.9   </td><td>1.7   </td><td>0.4   </td><td>setosa</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       " Sepal.Length & Sepal.Width & Petal.Length & Petal.Width & Species\\\\\n",
       "\\hline\n",
       "\t 5.1    & 3.5    & 1.4    & 0.2    & setosa\\\\\n",
       "\t 4.9    & 3.0    & 1.4    & 0.2    & setosa\\\\\n",
       "\t 4.7    & 3.2    & 1.3    & 0.2    & setosa\\\\\n",
       "\t 4.6    & 3.1    & 1.5    & 0.2    & setosa\\\\\n",
       "\t 5.0    & 3.6    & 1.4    & 0.2    & setosa\\\\\n",
       "\t 5.4    & 3.9    & 1.7    & 0.4    & setosa\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "Sepal.Length | Sepal.Width | Petal.Length | Petal.Width | Species | \n",
       "|---|---|---|---|---|---|\n",
       "| 5.1    | 3.5    | 1.4    | 0.2    | setosa | \n",
       "| 4.9    | 3.0    | 1.4    | 0.2    | setosa | \n",
       "| 4.7    | 3.2    | 1.3    | 0.2    | setosa | \n",
       "| 4.6    | 3.1    | 1.5    | 0.2    | setosa | \n",
       "| 5.0    | 3.6    | 1.4    | 0.2    | setosa | \n",
       "| 5.4    | 3.9    | 1.7    | 0.4    | setosa | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n",
       "1 5.1          3.5         1.4          0.2         setosa \n",
       "2 4.9          3.0         1.4          0.2         setosa \n",
       "3 4.7          3.2         1.3          0.2         setosa \n",
       "4 4.6          3.1         1.5          0.2         setosa \n",
       "5 5.0          3.6         1.4          0.2         setosa \n",
       "6 5.4          3.9         1.7          0.4         setosa "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data <- datasets::iris\n",
    "dim(data)\n",
    "head(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t150 obs. of  5 variables:\n",
      " $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n",
      " $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n",
      " $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n",
      " $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n",
      " $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n"
     ]
    }
   ],
   "source": [
    "# Structure of data\n",
    "str(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n",
       " Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n",
       " 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n",
       " Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n",
       " Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n",
       " 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n",
       " Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n",
       "       Species  \n",
       " setosa    :50  \n",
       " versicolor:50  \n",
       " virginica :50  \n",
       "                \n",
       "                \n",
       "                "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Statistical sumary of data\n",
    "summary(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrplot\n",
    "# corrplot(data[,1:4],method = \"circle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying required transformations in data\n",
    "To work with keras package we need to convert the data to an array or a matrix.\n",
    "The matrix data elements should be of the same basic type but here we have target values that are of factor type and hence we   need to change this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>0  </td></tr>\n",
       "\t<tr><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>0  </td></tr>\n",
       "\t<tr><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>0  </td></tr>\n",
       "\t<tr><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>0  </td></tr>\n",
       "\t<tr><td>5.0</td><td>3.6</td><td>1.4</td><td>0.2</td><td>0  </td></tr>\n",
       "\t<tr><td>5.4</td><td>3.9</td><td>1.7</td><td>0.4</td><td>0  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{lllll}\n",
       "\t 5.1 & 3.5 & 1.4 & 0.2 & 0  \\\\\n",
       "\t 4.9 & 3.0 & 1.4 & 0.2 & 0  \\\\\n",
       "\t 4.7 & 3.2 & 1.3 & 0.2 & 0  \\\\\n",
       "\t 4.6 & 3.1 & 1.5 & 0.2 & 0  \\\\\n",
       "\t 5.0 & 3.6 & 1.4 & 0.2 & 0  \\\\\n",
       "\t 5.4 & 3.9 & 1.7 & 0.4 & 0  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| 5.1 | 3.5 | 1.4 | 0.2 | 0   | \n",
       "| 4.9 | 3.0 | 1.4 | 0.2 | 0   | \n",
       "| 4.7 | 3.2 | 1.3 | 0.2 | 0   | \n",
       "| 4.6 | 3.1 | 1.5 | 0.2 | 0   | \n",
       "| 5.0 | 3.6 | 1.4 | 0.2 | 0   | \n",
       "| 5.4 | 3.9 | 1.7 | 0.4 | 0   | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     [,1] [,2] [,3] [,4] [,5]\n",
       "[1,] 5.1  3.5  1.4  0.2  0   \n",
       "[2,] 4.9  3.0  1.4  0.2  0   \n",
       "[3,] 4.7  3.2  1.3  0.2  0   \n",
       "[4,] 4.6  3.1  1.5  0.2  0   \n",
       "[5,] 5.0  3.6  1.4  0.2  0   \n",
       "[6,] 5.4  3.9  1.7  0.4  0   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "data[,5] <- as.numeric(data[,5]) -1\n",
    "\n",
    "# Turn data into a matrix\n",
    "data <- as.matrix(data)\n",
    "\n",
    "# Setting dimnames of data to NULL\n",
    "dimnames(data) <- NULL\n",
    "head(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the data into training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1</li>\n",
       "\t<li>1</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\item 2\n",
       "\\item 1\n",
       "\\item 1\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1\n",
       "2. 2\n",
       "3. 1\n",
       "4. 1\n",
       "5. 1\n",
       "6. 2\n",
       "7. 1\n",
       "8. 2\n",
       "9. 1\n",
       "10. 1\n",
       "11. 1\n",
       "12. 2\n",
       "13. 2\n",
       "14. 2\n",
       "15. 1\n",
       "16. 1\n",
       "17. 1\n",
       "18. 1\n",
       "19. 1\n",
       "20. 2\n",
       "21. 1\n",
       "22. 1\n",
       "23. 1\n",
       "24. 2\n",
       "25. 1\n",
       "26. 2\n",
       "27. 1\n",
       "28. 1\n",
       "29. 1\n",
       "30. 1\n",
       "31. 1\n",
       "32. 1\n",
       "33. 2\n",
       "34. 2\n",
       "35. 1\n",
       "36. 2\n",
       "37. 1\n",
       "38. 1\n",
       "39. 1\n",
       "40. 1\n",
       "41. 1\n",
       "42. 2\n",
       "43. 1\n",
       "44. 2\n",
       "45. 2\n",
       "46. 2\n",
       "47. 2\n",
       "48. 1\n",
       "49. 1\n",
       "50. 1\n",
       "51. 1\n",
       "52. 1\n",
       "53. 1\n",
       "54. 1\n",
       "55. 2\n",
       "56. 2\n",
       "57. 2\n",
       "58. 1\n",
       "59. 2\n",
       "60. 2\n",
       "61. 2\n",
       "62. 1\n",
       "63. 1\n",
       "64. 1\n",
       "65. 1\n",
       "66. 1\n",
       "67. 1\n",
       "68. 1\n",
       "69. 2\n",
       "70. 2\n",
       "71. 1\n",
       "72. 1\n",
       "73. 1\n",
       "74. 1\n",
       "75. 1\n",
       "76. 2\n",
       "77. 2\n",
       "78. 1\n",
       "79. 1\n",
       "80. 1\n",
       "81. 2\n",
       "82. 1\n",
       "83. 1\n",
       "84. 2\n",
       "85. 1\n",
       "86. 2\n",
       "87. 1\n",
       "88. 1\n",
       "89. 1\n",
       "90. 2\n",
       "91. 2\n",
       "92. 2\n",
       "93. 2\n",
       "94. 1\n",
       "95. 1\n",
       "96. 2\n",
       "97. 1\n",
       "98. 1\n",
       "99. 1\n",
       "100. 1\n",
       "101. 1\n",
       "102. 1\n",
       "103. 2\n",
       "104. 1\n",
       "105. 1\n",
       "106. 1\n",
       "107. 1\n",
       "108. 2\n",
       "109. 2\n",
       "110. 2\n",
       "111. 1\n",
       "112. 1\n",
       "113. 1\n",
       "114. 1\n",
       "115. 1\n",
       "116. 1\n",
       "117. 2\n",
       "118. 1\n",
       "119. 2\n",
       "120. 2\n",
       "121. 1\n",
       "122. 1\n",
       "123. 2\n",
       "124. 1\n",
       "125. 2\n",
       "126. 1\n",
       "127. 1\n",
       "128. 2\n",
       "129. 1\n",
       "130. 1\n",
       "131. 2\n",
       "132. 2\n",
       "133. 2\n",
       "134. 1\n",
       "135. 1\n",
       "136. 1\n",
       "137. 1\n",
       "138. 1\n",
       "139. 2\n",
       "140. 1\n",
       "141. 1\n",
       "142. 2\n",
       "143. 1\n",
       "144. 2\n",
       "145. 2\n",
       "146. 1\n",
       "147. 1\n",
       "148. 2\n",
       "149. 1\n",
       "150. 1\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  [1] 1 2 1 1 1 2 1 2 1 1 1 2 2 2 1 1 1 1 1 2 1 1 1 2 1 2 1 1 1 1 1 1 2 2 1 2 1\n",
       " [38] 1 1 1 1 2 1 2 2 2 2 1 1 1 1 1 1 1 2 2 2 1 2 2 2 1 1 1 1 1 1 1 2 2 1 1 1 1\n",
       " [75] 1 2 2 1 1 1 2 1 1 2 1 2 1 1 1 2 2 2 2 1 1 2 1 1 1 1 1 1 2 1 1 1 1 2 2 2 1\n",
       "[112] 1 1 1 1 1 2 1 2 2 1 1 2 1 2 1 1 2 1 1 2 2 2 1 1 1 1 1 2 1 1 2 1 2 2 1 1 2\n",
       "[149] 1 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setting seed to avoid random values each time the code is run ;\n",
    "set.seed(76)\n",
    "# Training and testing data sample size\n",
    "indexes <- sample(2,nrow(data),replace = TRUE,prob = c(0.67,0.33))\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the `iris` data\n",
    "data.train <- data[indexes==1, 1:4]\n",
    "data.test <- data[indexes==2, 1:4]\n",
    "\n",
    "# Split the class attribute\n",
    "data.trainingtarget <- data[indexes==1, 5]\n",
    "data.testtarget <- data[indexes==2, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding for target variable\n",
    "While building a multi class classification model in neural networks it is recommended to \n",
    "transform your target attribute from a vector that contains values for each class value to a matrix \n",
    "with a boolean for each class value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2\n",
      "[39] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"
     ]
    }
   ],
   "source": [
    "# One hot encoding of training and test data target values\n",
    "data.trainLabels <- to_categorical(data.trainingtarget)\n",
    "\n",
    "# One hot encode test target values\n",
    "data.testLabels <- to_categorical(data.testtarget)\n",
    "\n",
    "# Print out the test labels to double check the result\n",
    "print(data.testtarget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a sequential model\n",
    "model <- keras_model_sequential() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add layers to the model\n",
    "model %>% \n",
    "    layer_dense(units = 8, activation = 'relu', input_shape = ncol(data.train)) %>% \n",
    "    layer_dense(units = 5, activation = 'relu') %>% \n",
    "    layer_dense(units = 3, activation = 'softmax')\n",
    "\n",
    "# The output layer creates 3 output values, one for each Iris class (versicolor, virginica or setosa). The first layer, which contains 8 hidden notes, on the other hand, has an input_shape of 4.\n",
    "# This is because your training data data.train has 4 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can further inspect your model with the following functions:\n",
    "\n",
    "summary() function  - print a summary representation of your model;<br>\n",
    "get_config() - return a list that contains the configuration of the model;<br>\n",
    "get_layer() - return the layer configuration.<br>\n",
    "layers attribute - used to retrieve a flattened list of the model’s layers;<br>\n",
    "\n",
    "To list the input tensors, you can use the inputs attribute; and <br>\n",
    "To retrieve the output tensors, you can make use of the outputs attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Layer (type)                        Output Shape                    Param #     \n",
      "================================================================================\n",
      "dense_7 (Dense)                     (None, 8)                       40          \n",
      "________________________________________________________________________________\n",
      "dense_8 (Dense)                     (None, 5)                       45          \n",
      "________________________________________________________________________________\n",
      "dense_9 (Dense)                     (None, 3)                       18          \n",
      "================================================================================\n",
      "Total params: 103\n",
      "Trainable params: 103\n",
      "Non-trainable params: 0\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips: Explain get config what it returns and what does it mean and where it is used and extend it for other functions below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'sequential_3', 'layers': [{'class_name': 'Dense', 'config': {'name': 'dense_7', 'trainable': True, 'batch_input_shape': (None, 4), 'dtype': 'float32', 'units': 8, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_8', 'trainable': True, 'units': 5, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_9', 'trainable': True, 'units': 3, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get model configuration\n",
    "get_config(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Dense>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get layer configuration\n",
    "get_layer(model, index = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1]]\n",
       "<keras.layers.core.Dense>\n",
       "\n",
       "[[2]]\n",
       "<keras.layers.core.Dense>\n",
       "\n",
       "[[3]]\n",
       "<keras.layers.core.Dense>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List the model's layers\n",
    "model$layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1]]\n",
       "Tensor(\"dense_7_input:0\", shape=(?, 4), dtype=float32)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List the input tensors\n",
    "model$inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1]]\n",
       "Tensor(\"dense_9/Softmax:0\", shape=(?, 3), dtype=float32)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List the output tensors\n",
    "model$outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile And Fit The Model\n",
    "\n",
    "Some common optimization algorithms used are the Stochastic Gradient Descent (SGD), ADAM and RMSprop.<br>\n",
    "Depending on whichever algorithm you choose, you’ll need to tune certain parameters, such as learning rate or momentum.<br>\n",
    "The choice for a loss function depends on the task that you have at hand: for example, for a classsification problem, you’ll usually use cross entropy and for a binary classification problem we use binary_crossentropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model %>% compile(\n",
    "     loss = 'categorical_crossentropy',\n",
    "     optimizer = 'adam',\n",
    "     metrics = 'accuracy'\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model \n",
    "model %>% fit(\n",
    "     data.train, \n",
    "     data.trainLabels, \n",
    "     epochs = 200, \n",
    "     batch_size = 5, \n",
    "     validation_split = 0.2\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history <- model %>% fit(\n",
    "     data.train, \n",
    "     data.trainLabels, \n",
    "     epochs = 200,\n",
    "     batch_size = 5, \n",
    "     validation_split = 0.2\n",
    " )\n",
    "\n",
    "# Plot the history\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss and acc indicate the loss and accuracy of the model for the training data, while the val_loss and val_acc are the same metrics, loss and accuracy, for the test or validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the classes for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes <- model %>% predict_classes(data.test, batch_size = 128)\n",
    "\n",
    "# Confusion matrix\n",
    "table(data.testtarget, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data and labels\n",
    "score <- model %>% evaluate(data.test, data.testLabels, batch_size = 128)\n",
    "\n",
    "# Print the score\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
