{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning is the process of maximizing a model’s performance without overfitting or underfitting, this can be achived by setting appropriate values to model parameters. A deep neural network has following parameters which we can tune , Layers, hidden units\n",
    "optimization paramters like optimizer used, learning rate and number of epochs.\n",
    "\n",
    "\n",
    "To tune your Keras model parameters we first to define flags for parameters which we want to optimise. It is defined by flags() function of keras package which returns a object of type \"tfruns_flags\" which contains information of parameters that we want to tune. Below we have declared four flags that control dropout rate and the number of neurons in the first and second layer of the model. \"flag_integer(\"dense_units1\",8)\" flag tunes number of units in 1 layer, \"dense_units1\" is the name of the flag and 8 is the default value. Once we have defined flags, we use these flags in the definition of our model. In the below code, we have defined our model and with parameters which we want to tune. Now we would save this in a file \"hypereparameter_tuning_model.R\", this script does not tune your hyperparameters, it just defines paramaterized training runs to create the best model. We will tune our network in script \"tuning.R\", discussed after the below code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(keras)\n",
    "library(datasets)\n",
    "\n",
    "mnist <- dataset_mnist()\n",
    "x_train <- mnist$train$x\n",
    "y_train <- mnist$train$y\n",
    "x_test <- mnist$test$x\n",
    "y_test <- mnist$test$y\n",
    "\n",
    "# reshape\n",
    "x_train <- array_reshape(x_train, c(nrow(x_train), 784))\n",
    "x_test <- array_reshape(x_test, c(nrow(x_test), 784))\n",
    "\n",
    "# rescale\n",
    "x_train <- x_train / 255\n",
    "x_test <- x_test / 255\n",
    "\n",
    "y_train <- to_categorical(y_train, 10)\n",
    "y_test <- to_categorical(y_test, 10)\n",
    "\n",
    "\n",
    "# Defining flags\n",
    "FLAGS <- flags(\n",
    "    flag_integer(\"dense_units1\",8),\n",
    "    flag_numeric(\"dropout1\",0.4),\n",
    "    flag_integer(\"dense_units2\",8),\n",
    "    flag_numeric(\"dropout2\", 0.3)\n",
    ")\n",
    "\n",
    "model <- keras_model_sequential()\n",
    "model %>%\n",
    "  layer_dense(units = FLAGS$dense_units1, activation = 'relu', input_shape = c(784)) %>%\n",
    "  layer_dropout(rate = FLAGS$dropout1) %>%\n",
    "  layer_dense(units = FLAGS$dense_units2, activation = 'relu') %>%\n",
    "  layer_dropout(rate = FLAGS$dropout2) %>%\n",
    "  layer_dense(units = 10, activation = 'softmax')\n",
    "\n",
    "model %>% compile(\n",
    "  loss = 'categorical_crossentropy',\n",
    "  optimizer = optimizer_rmsprop(lr = 0.001),\n",
    "  metrics = c('accuracy')\n",
    ")\n",
    "\n",
    "# Training & Evaluation ----------------------------------------------------\n",
    "\n",
    "history <- model %>% fit(\n",
    "  x_train, y_train,\n",
    "  batch_size = 128,\n",
    "  epochs = 20,\n",
    "  verbose = 1,\n",
    "  validation_split = 0.2\n",
    ")\n",
    "\n",
    "plot(history)\n",
    "\n",
    "score <- model %>% evaluate(\n",
    "  x_test, y_test,\n",
    "  verbose = 0\n",
    ")\n",
    "\n",
    "cat('Test loss:', score$loss, '\\n')\n",
    "cat('Test accuracy:', score$acc, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In tuning.R script we have used tuning_run() function from package tfruns.The \"tfruns\" package provides a suite of tools for tracking, visualizing, and managing TensorFlow training runs and experiments from R. Argument \"file\" of the function takes  path of the file which we have described above i.e. \"hypereparameter_tuning_model.R\", \"flags\" argument takes as a list of key value pair where key names must match the names of different flags defined in our model. tuning_run() runs executes training runs for all combinations of the specified flags and by default all runs go into the “runs” sub-directory of the current working directory.It returns a data frame which contains summary information of all runs like evaluation ,validation and performance  loss(categorical_crossentropy) and metric(accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'tfruns' was built under R version 3.5.3\""
     ]
    }
   ],
   "source": [
    "library(tfruns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs <- tuning_run(file = \"hypereparameter_tuning_model.R\", flags = list(\n",
    "  dense_units1 = c(8,16),\n",
    "  dropout1 = c(0.2, 0.3, 0.4),\n",
    "  dense_units2 = c(8,16),\n",
    "  dropout2 = c(0.2, 0.3, 0.4)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
